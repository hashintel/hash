version: "3.9"

volumes:
  hash-postgres-data:
  log:

services:
  hash-dev-opensearch:
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      HASH_OPENSEARCH_ENABLED: "${HASH_OPENSEARCH_ENABLED}"
      ## Tell OpenSearch that it's operating in single-node mode
      discovery.type: single-node
      ## Disable the security module for development so we can connect over plain HTTP
      plugins.security.disabled: true
      ## Docker volumes are ~10GB by default which is typically much smaller than the
      ## host's drive size. This can cause OpenSearch to shutdown if it thinks disk
      ## space is running low. Set the disk high watermark to 100% to ignore this.
      cluster.routing.allocation.disk.watermark.high: 100%
      cluster.routing.allocation.disk.watermark.flood_stage: 100%
    build:
      context: ./opensearch
    ports:
      - "9200:9200"
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    ## Mounting open search data to a local directory may lead to java.nio.file.AccessDeniedException.
    ## Details: https://github.com/opensearch-project/OpenSearch/issues/1579.
    ## We can revisit the setup after upgrading base image or by fixing permissions in a custom image.
    # volumes:
    #   - ../../var/hash-external-service/opensearch/data:/usr/share/opensearch/data

  postgres:
    build:
      context: ./postgres
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      PGDATA: /data/pgdata
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      HASH_KRATOS_PG_USER: "${HASH_KRATOS_PG_USER}"
      HASH_KRATOS_PG_PASSWORD: "${HASH_KRATOS_PG_PASSWORD}"
      HASH_KRATOS_PG_DATABASE: "${HASH_KRATOS_PG_DEV_DATABASE}"
      HASH_TEMPORAL_PG_USER: "${HASH_TEMPORAL_PG_USER}"
      HASH_TEMPORAL_PG_PASSWORD: "${HASH_TEMPORAL_PG_PASSWORD}"
      HASH_TEMPORAL_PG_DATABASE: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      HASH_TEMPORAL_VISIBILITY_PG_DATABASE: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      HASH_GRAPH_PG_USER: "${HASH_GRAPH_PG_USER}"
      HASH_GRAPH_PG_PASSWORD: "${HASH_GRAPH_PG_PASSWORD}"
      HASH_GRAPH_PG_DATABASE: "${HASH_GRAPH_PG_DEV_DATABASE}"
      HASH_GRAPH_REALTIME_PG_USER: "${HASH_GRAPH_REALTIME_PG_USER}"
      HASH_GRAPH_REALTIME_PG_PASSWORD: "${HASH_GRAPH_REALTIME_PG_PASSWORD}"
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - hash-postgres-data:/var/lib/postgresql/data
      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./postgres/init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready --username ${POSTGRES_USER}"]
      interval: 2s
      timeout: 2s
      retries: 5
    command: -c 'config_file=/etc/postgresql/postgresql.conf'

  graph-migrate:
    init: true
    depends_on:
      postgres:
        condition: service_healthy
      telemetry-collector:
        condition: service_healthy
    image: hash-graph
    read_only: true
    security_opt:
      - no-new-privileges:true
    volumes:
      - log:/log
    command: migrate
    environment:
      # Intentionally use the POSTGRES user as it's the "superadmin" which has access to schema
      HASH_GRAPH_PG_USER: "${POSTGRES_USER}"
      HASH_GRAPH_PG_PASSWORD: "${POSTGRES_PASSWORD}"
      HASH_GRAPH_PG_HOST: "postgres"
      HASH_GRAPH_PG_PORT: "5432"
      HASH_GRAPH_PG_DATABASE: "${HASH_GRAPH_PG_DEV_DATABASE}"
      HASH_GRAPH_LOG_FORMAT: "${HASH_GRAPH_LOG_FORMAT:-pretty}"
      HASH_GRAPH_LOG_FOLDER: "/log/graph-migrations"
      # For unknown reasons, our error return values are consumed when we
      # configure the OTLP endpoint for the Graph. For now, we've disabled traces.
      # https://app.asana.com/0/1201095311341924/1203636955054323/f
      # HASH_GRAPH_OTLP_ENDPOINT: "http://telemetry-collector:4317"
      RUST_LOG: "${HASH_GRAPH_LOG_LEVEL:-graph=trace,hash-graph=trace,tokio_postgres=debug}"
      RUST_BACKTRACE: 1

  telemetry-collector:
    image: jaegertracing/all-in-one:1.40
    deploy:
      restart_policy:
        condition: on-failure
    healthcheck:
      # Port 14269 is the Jaeger admin endpoint
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:14269 || exit 1",
        ]
      interval: 2s
      timeout: 2s
      retries: 10
    ports:
      - "16686:16686"
      # To expose OTLP collector over gRPC on the host
      - "4317:4317"
      # To expose OTLP collector over HTTP on the host
      # - 4318:4318
    environment:
      COLLECTOR_OTLP_ENABLED: "true"

  graph:
    init: true
    depends_on:
      postgres:
        condition: service_healthy
      graph-migrate:
        condition: service_completed_successfully
      telemetry-collector:
        condition: service_healthy
    image: hash-graph
    read_only: true
    security_opt:
      - no-new-privileges:true
    volumes:
      - log:/log
    command: server
    environment:
      HASH_GRAPH_PG_USER: "${HASH_GRAPH_PG_USER}"
      HASH_GRAPH_PG_PASSWORD: "${HASH_GRAPH_PG_PASSWORD}"
      HASH_GRAPH_PG_HOST: "postgres"
      HASH_GRAPH_PG_PORT: "5432"
      HASH_GRAPH_PG_DATABASE: "${HASH_GRAPH_PG_DEV_DATABASE}"
      HASH_GRAPH_LOG_FORMAT: "${HASH_GRAPH_LOG_FORMAT:-pretty}"
      HASH_GRAPH_LOG_FOLDER: "/log/graph-service"
      HASH_GRAPH_TYPE_FETCHER_HOST: "type-fetcher"
      HASH_GRAPH_TYPE_FETCHER_PORT: "${HASH_GRAPH_TYPE_FETCHER_PORT}"
      HASH_GRAPH_API_HOST: "0.0.0.0"
      HASH_GRAPH_API_PORT: "4000"
      # For unknown reasons, our error return values are consumed when we
      # configure the OTLP endpoint for the Graph. For now, we've disabled traces.
      # https://app.asana.com/0/1201095311341924/1203636955054323/f
      # HASH_GRAPH_OTLP_ENDPOINT: "http://telemetry-collector:4317"
      RUST_LOG: "${HASH_GRAPH_LOG_LEVEL:-info,graph=trace,hash_graph=trace,tokio_postgres=debug}"
      RUST_BACKTRACE: 1
    ports:
      - "${HASH_GRAPH_API_PORT}:4000"
    healthcheck:
      test:
        ["CMD", "/hash-graph", "server", "--healthcheck", "--api-port", "4000"]
      interval: 2s
      timeout: 2s
      retries: 10

  hash-dev-redis:
    image: redis:6.2
    deploy:
      restart_policy:
        condition: on-failure
    ports:
      - "6379:6379"

  kratos-migrate:
    build:
      context: ./kratos
      args:
        ENV: dev
        SECRET: "${KRATOS_API_KEY}"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DSN=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/${HASH_KRATOS_PG_DEV_DATABASE}
    command: migrate sql -e --yes

  kratos:
    build:
      context: ./kratos
      args:
        ENV: dev
        SECRET: "${KRATOS_API_KEY}"
    depends_on:
      kratos-migrate:
        condition: service_completed_successfully
    ports:
      - "4433:4433" # public
      - "4434:4434" # admin
    restart: unless-stopped
    environment:
      SECRETS_COOKIE: "${KRATOS_SECRETS_COOKIE}"
      SECRETS_CIPHER: "${KRATOS_SECRETS_CIPHER}"
      COURIER_SMTP_CONNECTION_URI: "smtps://test:test@mailslurper:1025/?skip_ssl_verify=true"
      DSN: "postgres://${HASH_KRATOS_PG_USER}:${HASH_KRATOS_PG_PASSWORD}@postgres:${POSTGRES_PORT}/${HASH_KRATOS_PG_DEV_DATABASE}"
      LOG_LEVEL: trace
    command: serve --dev --watch-courier
    extra_hosts:
      - host.docker.internal:host-gateway

  mailslurper:
    image: oryd/mailslurper:latest-smtps
    ports:
      - "4436:4436"
      - "4437:4437"

  hash-agents:
    init: true
    image: hash-agents
    read_only: true
    security_opt:
      - no-new-privileges:true
    volumes:
      - log:/log
      # Mount the agents folder from the host so that we can edit the agents without having to rebuild the image.
      - ../../apps/hash-agents/app/agents:/usr/local/src/apps/hash-agents/app/agents:readonly
    tmpfs:
      - /tmp
    environment:
      HASH_AGENT_RUNNER_LOG_FOLDER: "/log/hash-agents"
      HASH_AGENT_RUNNER_LOG_LEVEL: "${HASH_AGENT_RUNNER_LOG_LEVEL}"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
    ports:
      - "${HASH_AGENT_RUNNER_PORT}:80"

  temporal-migrate:
    build:
      context: ./temporal
      dockerfile: migrate.Dockerfile
    read_only: true
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DB: "postgresql"
      DBNAME: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      VISIBILITY_DBNAME: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      DB_PORT: "5432"
      # Intentionally use the POSTGRES user as it's the "superadmin" which has access to schema
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PWD: "${POSTGRES_PASSWORD}"
      POSTGRES_SEEDS: "postgres" # the hostname of the postgres container
      ENABLE_ES: "${HASH_OPENSEARCH_ENABLED}"
      ES_SEEDS: "hash-dev-opensearch" # the hostname of the opensearch container
      ES_VERSION: "v7"
    security_opt:
      - no-new-privileges:true

  temporal-setup:
    build:
      context: ./temporal
      dockerfile: setup.Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
      temporal:
        condition: service_started
    environment:
      DB: "postgresql"
      DBNAME: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      VISIBILITY_DBNAME: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      DB_PORT: "5432"
      # Intentionally use the POSTGRES user as it's the "superadmin" which has access to schema
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PWD: "${POSTGRES_PASSWORD}"
      POSTGRES_SEEDS: "postgres" # the hostname of the postgres container
      ENABLE_ES: "${HASH_OPENSEARCH_ENABLED}"
      ES_SEEDS: "hash-dev-opensearch" # the hostname of the opensearch container
      ES_VERSION: "v7"
      TEMPORAL_CLI_ADDRESS: temporal:7233
      SKIP_DEFAULT_NAMESPACE_CREATION: "false" # left as a convenience as most temporal tooling expects default namespace
    security_opt:
      - no-new-privileges:true

  temporal:
    container_name: temporal
    image: "temporalio/server:1.20.2.0"
    deploy:
      restart_policy:
        condition: on-failure
    depends_on:
      postgres:
        condition: service_healthy
      temporal-migrate:
        condition: service_completed_successfully
      hash-dev-opensearch:
        condition: service_started
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "workflow", "list"]
      interval: 10s
      timeout: 2s
      retries: 10
    environment:
      DB: "postgresql"
      DBNAME: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      VISIBILITY_DBNAME: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      DB_PORT: "5432"
      POSTGRES_USER: "${HASH_TEMPORAL_PG_USER}"
      POSTGRES_PWD: "${HASH_TEMPORAL_PG_PASSWORD}"
      POSTGRES_SEEDS: "postgres" # the hostname of the postgres container
      #     DYNAMIC_CONFIG_FILE_PATH: "config/dynamicconfig/development-sql.yaml"
      ENABLE_ES: "${HASH_OPENSEARCH_ENABLED}"
      ES_SEEDS: "hash-dev-opensearch" # the hostname of the opensearch container
      ES_VERSION: "v7"
    security_opt:
      - no-new-privileges:true
    ports:
      - "${HASH_TEMPORAL_SERVER_PORT}:7233"
  #    volumes:
  #      - ./dynamicconfig:/etc/temporal/config/dynamicconfig

  temporal-ui:
    image: temporalio/ui:2.13.3
    container_name: temporal-ui
    deploy:
      restart_policy:
        condition: on-failure
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      TEMPORAL_ADDRESS: temporal:7233
      TEMPORAL_CORS_ORIGINS: http://localhost:3000
    security_opt:
      - no-new-privileges:true
    ports:
      - "${HASH_TEMPORAL_UI_PORT}:8080"

  hash-temporal-worker-ts:
    image: hash-ai-worker-ts
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_healthy
    # Environment variables are to be set manually here.
    # We could load in the entirety of the `.env.local` file using `env_file`
    # but to prevent bloating the environment and spilling everything
    # into the process, it is constrained here.
    environment:
      HASH_TEMPORAL_HOST: "temporal"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
    tmpfs:
      - /tmp
    read_only: true
    security_opt:
      - no-new-privileges:true

  hash-temporal-worker-py:
    image: hash-ai-worker-py
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_healthy
    # Environment variables are to be set manually here.
    # We could load in the entirety of the `.env.local` file using `env_file`
    # but to prevent bloating the environment and spilling everything
    # into the process, it is constrained here.
    environment:
      HASH_TEMPORAL_HOST: "temporal"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
    tmpfs:
      - /tmp
    read_only: true
    security_opt:
      - no-new-privileges:true
