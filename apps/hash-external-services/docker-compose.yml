version: "3.9"

volumes:
  hash-postgres-data:
  hash-vault-data:
  logs:

services:
  hash-dev-opensearch:
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      HASH_OPENSEARCH_ENABLED: "${HASH_OPENSEARCH_ENABLED}"
      ## Tell OpenSearch that it's operating in single-node mode
      discovery.type: single-node
      ## Disable the security module for development so we can connect over plain HTTP
      plugins.security.disabled: true
      ## Docker volumes are ~10GB by default which is typically much smaller than the
      ## host's drive size. This can cause OpenSearch to shutdown if it thinks disk
      ## space is running low. Set the disk high watermark to 100% to ignore this.
      cluster.routing.allocation.disk.watermark.high: 100%
      cluster.routing.allocation.disk.watermark.flood_stage: 100%
    build:
      context: ./opensearch
    ports:
      - "9200:9200"
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    ## Mounting open search data to a local directory may lead to java.nio.file.AccessDeniedException.
    ## Details: https://github.com/opensearch-project/OpenSearch/issues/1579.
    ## We can revisit the setup after upgrading base image or by fixing permissions in a custom image.
    # volumes:
    #   - ../../var/hash-external-service/opensearch/data:/usr/share/opensearch/data

  postgres:
    build:
      context: ./postgres
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      PGDATA: /data/pgdata
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      HASH_KRATOS_PG_USER: "${HASH_KRATOS_PG_USER}"
      HASH_KRATOS_PG_PASSWORD: "${HASH_KRATOS_PG_PASSWORD}"
      HASH_KRATOS_PG_DATABASE: "${HASH_KRATOS_PG_DEV_DATABASE}"
      HASH_TEMPORAL_PG_USER: "${HASH_TEMPORAL_PG_USER}"
      HASH_TEMPORAL_PG_PASSWORD: "${HASH_TEMPORAL_PG_PASSWORD}"
      HASH_TEMPORAL_PG_DATABASE: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      HASH_TEMPORAL_VISIBILITY_PG_DATABASE: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      HASH_GRAPH_PG_USER: "${HASH_GRAPH_PG_USER}"
      HASH_GRAPH_PG_PASSWORD: "${HASH_GRAPH_PG_PASSWORD}"
      HASH_GRAPH_PG_DATABASE: "${HASH_GRAPH_PG_DEV_DATABASE}"
      HASH_GRAPH_REALTIME_PG_USER: "${HASH_GRAPH_REALTIME_PG_USER}"
      HASH_GRAPH_REALTIME_PG_PASSWORD: "${HASH_GRAPH_REALTIME_PG_PASSWORD}"
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - hash-postgres-data:/var/lib/postgresql/data
      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./postgres/init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready --username ${POSTGRES_USER}"]
      interval: 2s
      timeout: 2s
      retries: 5
    command: -c 'config_file=/etc/postgresql/postgresql.conf'

  graph-migrate:
    init: true
    depends_on:
      postgres:
        condition: service_healthy
      telemetry-collector:
        condition: service_healthy
    image: hash-graph
    read_only: true
    security_opt:
      - no-new-privileges:true
    volumes:
      - logs:/logs
    command: migrate
    environment:
      # Intentionally use the POSTGRES user as it's the "superadmin" which has access to schema
      HASH_GRAPH_PG_USER: "${POSTGRES_USER}"
      HASH_GRAPH_PG_PASSWORD: "${POSTGRES_PASSWORD}"
      HASH_GRAPH_PG_HOST: "postgres"
      HASH_GRAPH_PG_PORT: "5432"
      HASH_GRAPH_PG_DATABASE: "${HASH_GRAPH_PG_DEV_DATABASE}"
      HASH_GRAPH_LOG_FORMAT: "${HASH_GRAPH_LOG_FORMAT:-pretty}"
      HASH_GRAPH_LOG_FOLDER: "/logs/graph-migrations"
      # For unknown reasons, our error return values are consumed when we
      # configure the OTLP endpoint for the Graph. For now, we've disabled traces.
      # https://app.asana.com/0/1201095311341924/1203636955054323/f
      # HASH_GRAPH_OTLP_ENDPOINT: "http://telemetry-collector:4317"
      RUST_LOG: "${HASH_GRAPH_LOG_LEVEL:-graph=trace,hash-graph=trace,tokio_postgres=debug}"
      RUST_BACKTRACE: 1

  telemetry-collector:
    image: jaegertracing/all-in-one:1.40
    deploy:
      restart_policy:
        condition: on-failure
    healthcheck:
      # Port 14269 is the Jaeger admin endpoint
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider http://localhost:14269 || exit 1",
        ]
      interval: 2s
      timeout: 2s
      retries: 10
    ports:
      - "16686:16686"
      # To expose OTLP collector over gRPC on the host
      - "4317:4317"
      # To expose OTLP collector over HTTP on the host
      # - 4318:4318
    environment:
      COLLECTOR_OTLP_ENABLED: "true"

  graph:
    init: true
    depends_on:
      postgres:
        condition: service_healthy
      graph-migrate:
        condition: service_completed_successfully
      telemetry-collector:
        condition: service_healthy
    image: hash-graph
    read_only: true
    security_opt:
      - no-new-privileges:true
    volumes:
      - logs:/logs
    command: server --offline
    environment:
      HASH_GRAPH_PG_USER: "${HASH_GRAPH_PG_USER}"
      HASH_GRAPH_PG_PASSWORD: "${HASH_GRAPH_PG_PASSWORD}"
      HASH_GRAPH_PG_HOST: "postgres"
      HASH_GRAPH_PG_PORT: "5432"
      HASH_GRAPH_PG_DATABASE: "${HASH_GRAPH_PG_DEV_DATABASE}"
      HASH_GRAPH_LOG_FORMAT: "${HASH_GRAPH_LOG_FORMAT:-pretty}"
      HASH_GRAPH_LOG_FOLDER: "/logs/graph-service"
      HASH_GRAPH_TYPE_FETCHER_HOST: "type-fetcher"
      HASH_GRAPH_TYPE_FETCHER_PORT: "${HASH_GRAPH_TYPE_FETCHER_PORT}"
      HASH_GRAPH_API_HOST: "0.0.0.0"
      HASH_GRAPH_API_PORT: "4000"
      # For unknown reasons, our error return values are consumed when we
      # configure the OTLP endpoint for the Graph. For now, we've disabled traces.
      # https://app.asana.com/0/1201095311341924/1203636955054323/f
      # HASH_GRAPH_OTLP_ENDPOINT: "http://telemetry-collector:4317"
      RUST_LOG: "${HASH_GRAPH_LOG_LEVEL:-info,graph=trace,hash_graph=trace,tokio_postgres=debug}"
      RUST_BACKTRACE: 1
    ports:
      - "${HASH_GRAPH_API_PORT}:4000"
    healthcheck:
      test:
        ["CMD", "/hash-graph", "server", "--healthcheck", "--api-port", "4000"]
      interval: 2s
      timeout: 2s
      retries: 10

  hash-dev-redis:
    image: redis:6.2
    deploy:
      restart_policy:
        condition: on-failure
    ports:
      - "6379:6379"

  kratos-migrate:
    build:
      context: ./kratos
      args:
        ENV: dev
        SECRET: "${KRATOS_API_KEY}"
        API_CALLBACK_URL: "http://host.docker.internal:5001/kratos-after-registration"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DSN=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/${HASH_KRATOS_PG_DEV_DATABASE}
    command: migrate sql -e --yes

  kratos:
    build:
      context: ./kratos
      args:
        ENV: dev
        SECRET: "${KRATOS_API_KEY}"
        API_CALLBACK_URL: "http://host.docker.internal:5001/kratos-after-registration"
    depends_on:
      kratos-migrate:
        condition: service_completed_successfully
    ports:
      - "4433:4433" # public
      - "4434:4434" # admin
    restart: unless-stopped
    environment:
      SECRETS_COOKIE: "${KRATOS_SECRETS_COOKIE}"
      SECRETS_CIPHER: "${KRATOS_SECRETS_CIPHER}"
      COURIER_SMTP_CONNECTION_URI: "smtps://test:test@mailslurper:1025/?skip_ssl_verify=true"
      DSN: "postgres://${HASH_KRATOS_PG_USER}:${HASH_KRATOS_PG_PASSWORD}@postgres:${POSTGRES_PORT}/${HASH_KRATOS_PG_DEV_DATABASE}"
      LOG_LEVEL: trace
    command: serve --dev --watch-courier
    extra_hosts:
      - host.docker.internal:host-gateway

  mailslurper:
    image: oryd/mailslurper:latest-smtps
    ports:
      - "4436:4436"
      - "4437:4437"

  temporal-migrate:
    build:
      context: ./temporal
      dockerfile: migrate.Dockerfile
      args:
        TEMPORAL_VERSION: "${HASH_TEMPORAL_VERSION}"
    read_only: true
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # This sets configuration values in
      # https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml
      # posgres12 for v12+ of postgres.
      DB: "postgres12"
      DBNAME: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      VISIBILITY_DBNAME: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      DB_PORT: "5432"
      # Intentionally use the POSTGRES user as it's the "superadmin" which has access to schema
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PWD: "${POSTGRES_PASSWORD}"
      POSTGRES_SEEDS: "postgres" # the hostname of the postgres container
    security_opt:
      - no-new-privileges:true

  temporal:
    container_name: temporal
    image: "temporalio/server:${HASH_TEMPORAL_VERSION}"
    deploy:
      restart_policy:
        condition: on-failure
    depends_on:
      postgres:
        condition: service_healthy
      temporal-migrate:
        condition: service_completed_successfully
      hash-dev-opensearch:
        condition: service_started
    healthcheck:
      test:
        [
          "CMD",
          "temporal",
          "workflow",
          "list",
          "--namespace",
          "HASH",
          "--address",
          "temporal:7233",
        ]
      interval: 10s
      timeout: 2s
      retries: 10
    environment:
      # This sets configuration values in
      # https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml
      # posgres12 for v12+ of postgres.
      DB: "postgres12"
      DBNAME: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      VISIBILITY_DBNAME: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      DB_PORT: "5432"
      POSTGRES_USER: "${HASH_TEMPORAL_PG_USER}"
      POSTGRES_PWD: "${HASH_TEMPORAL_PG_PASSWORD}"
      POSTGRES_SEEDS: "postgres" # the hostname of the postgres container
    security_opt:
      - no-new-privileges:true
    ports:
      - "${HASH_TEMPORAL_SERVER_PORT}:7233"

  temporal-setup:
    build:
      context: ./temporal
      dockerfile: setup.Dockerfile
      args:
        TEMPORAL_VERSION: "${HASH_TEMPORAL_VERSION}"
    depends_on:
      postgres:
        condition: service_healthy
      temporal:
        condition: service_started
    environment:
      # This sets configuration values in
      # https://github.com/temporalio/temporal/blob/master/docker/config_template.yaml
      # posgres12 for v12+ of postgres.
      DB: "postgres12"
      DBNAME: "${HASH_TEMPORAL_PG_DEV_DATABASE}"
      VISIBILITY_DBNAME: "${HASH_TEMPORAL_VISIBILITY_PG_DEV_DATABASE}"
      DB_PORT: "5432"
      # Intentionally use the POSTGRES user as it's the "superadmin" which has access to schema
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PWD: "${POSTGRES_PASSWORD}"
      POSTGRES_SEEDS: "postgres" # the hostname of the postgres container
      TEMPORAL_ADDRESS: temporal:7233
      SKIP_DEFAULT_NAMESPACE_CREATION: "false" # left as a convenience as most temporal tooling expects default namespace
    security_opt:
      - no-new-privileges:true

  temporal-ui:
    image: temporalio/ui:${HASH_TEMPORAL_UI_VERSION}
    container_name: temporal-ui
    deploy:
      restart_policy:
        condition: on-failure
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      TEMPORAL_ADDRESS: temporal:7233
      TEMPORAL_CORS_ORIGINS: http://localhost:3000
    security_opt:
      - no-new-privileges:true
    ports:
      - "${HASH_TEMPORAL_UI_PORT}:8080"

  hash-temporal-worker-ts:
    image: hash-ai-worker-ts
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_healthy
    # Environment variables are to be set manually here.
    # We could load in the entirety of the `.env.local` file using `env_file`
    # but to prevent bloating the environment and spilling everything
    # into the process, it is constrained here.
    environment:
      HASH_TEMPORAL_HOST: "temporal"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
      HASH_GRAPH_API_HOST: graph
      HASH_GRAPH_API_PORT: "${HASH_GRAPH_API_PORT}"
      # TODO: Remove once they're not needed anymore
      #   see https://linear.app/hash/issue/H-55/avoid-ory-kratos-environment-to-be-required-for-hash-api
      ORY_KRATOS_PUBLIC_URL: "${ORY_KRATOS_PUBLIC_URL}"
      ORY_KRATOS_ADMIN_URL: "${ORY_KRATOS_ADMIN_URL}"
    tmpfs:
      - /tmp
    read_only: true
    security_opt:
      - no-new-privileges:true

  hash-temporal-worker-py:
    image: hash-ai-worker-py
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_healthy
    # Environment variables are to be set manually here.
    # We could load in the entirety of the `.env.local` file using `env_file`
    # but to prevent bloating the environment and spilling everything
    # into the process, it is constrained here.
    environment:
      HASH_TEMPORAL_HOST: "temporal"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
    tmpfs:
      - /tmp
    read_only: true
    security_opt:
      - no-new-privileges:true

  hash-temporal-worker-integration:
    image: hash-integration-worker
    restart: unless-stopped
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      HASH_TEMPORAL_HOST: "temporal"
      HASH_GRAPH_API_HOST: graph
      HASH_GRAPH_API_PORT: "${HASH_GRAPH_API_PORT}"
    tmpfs:
      - /tmp
    read_only: true
    security_opt:
      - no-new-privileges:true

  vault:
    image: hashicorp/vault
    ports:
      - "${HASH_VAULT_PORT}:8200"
    volumes:
      - hash-postgres-data:/vault/file:rw
      # - ./vault/config:/vault/config:rw - for when we need a config file
    cap_add:
      - IPC_LOCK
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: "${HASH_VAULT_ROOT_TOKEN}"
      VAULT_DEV_LISTEN_ADDRESS: "0.0.0.0:8200"
    security_opt:
      - no-new-privileges:true
