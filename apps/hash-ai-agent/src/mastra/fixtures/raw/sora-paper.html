<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</title>
<!--Generated on Mon Feb 26 20:42:26 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.17177v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S1" title="1 Introduction ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S2" title="2 Background ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S2.SS1" title="2.1 History ‣ 2 Background ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>History</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S2.SS2" title="2.2 Advanced Concepts ‣ 2 Background ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Advanced Concepts</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3" title="3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Technology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS1" title="3.1 Overview of Sora ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview of <span class="ltx_text ltx_font_typewriter">Sora</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2" title="3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Pre-processing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS1" title="3.2.1 Variable Durations, Resolutions, Aspect Ratios ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Variable Durations, Resolutions, Aspect Ratios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS2" title="3.2.2 Unified Visual Representation ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Unified Visual Representation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS3" title="3.2.3 Video Compression Network ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Video Compression Network</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS4" title="3.2.4 Spacetime Latent Patches ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>Spacetime Latent Patches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS5" title="3.2.5 Discussion ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS6" title="3.2.6 Diffusion Transformer ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.6 </span>Diffusion Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS3" title="3.3 Modeling ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS3.SSS1" title="3.3.1 Discussion ‣ 3.3 Modeling ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4" title="3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Language Instruction Following</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS1" title="3.4.1 Large Language Models ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS2" title="3.4.2 Text-to-Image ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Text-to-Image</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS3" title="3.4.3 Text-to-Video ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Text-to-Video</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS4" title="3.4.4 Discussion ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5" title="3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Prompt Engineering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS1" title="3.5.1 Text Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Text Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS2" title="3.5.2 Image Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>Image Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS3" title="3.5.3 Video Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.3 </span>Video Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS4" title="3.5.4 Discussion ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6" title="3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Trustworthiness</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS1" title="3.6.1 Safety Concern ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>Safety Concern</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS2" title="3.6.2 Other Exploitation ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>Other Exploitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS3" title="3.6.3 Alignment ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.3 </span>Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS4" title="3.6.4 Discussion ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.4 </span>Discussion</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4" title="4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS1" title="4.1 Movie ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Movie</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS2" title="4.2 Education ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Education</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS3" title="4.3 Gaming ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Gaming</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS4" title="4.4 Healthcare ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Healthcare</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS5" title="4.5 Robotics ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Robotics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S5" title="5 Discussion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S5.SS1" title="5.1 Limitations ‣ 5 Discussion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S5.SS2" title="5.2 Opportunities ‣ 5 Discussion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Opportunities</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S6" title="6 Conclusion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#A1" title="Appendix A Related Works ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Related Works</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: titletoc</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2402.17177v1 [cs.CV] 27 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_typewriter" id="id15.id1">Sora</span>: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.1">Yixin Liu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><msup id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml"><mi id="id1.1.1.m1.1.1a" xref="id1.1.1.m1.1.1.cmml"></mi><mn id="id1.1.1.m1.1.1.1" mathvariant="normal" xref="id1.1.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><apply id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"><cn id="id1.1.1.m1.1.1.1.cmml" type="integer" xref="id1.1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex1.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id2.2.2">Kai Zhang<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id2.2.2.m1.1"><semantics id="id2.2.2.m1.1a"><msup id="id2.2.2.m1.1.1" xref="id2.2.2.m1.1.1.cmml"><mi id="id2.2.2.m1.1.1a" xref="id2.2.2.m1.1.1.cmml"></mi><mn id="id2.2.2.m1.1.1.1" mathvariant="normal" xref="id2.2.2.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><apply id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1"><cn id="id2.2.2.m1.1.1.1.cmml" type="integer" xref="id2.2.2.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id2.2.2.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id3.3.3">Yuan Li<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id3.3.3.m1.1"><semantics id="id3.3.3.m1.1a"><msup id="id3.3.3.m1.1.1" xref="id3.3.3.m1.1.1.cmml"><mi id="id3.3.3.m1.1.1a" xref="id3.3.3.m1.1.1.cmml"></mi><mn id="id3.3.3.m1.1.1.1" mathvariant="normal" xref="id3.3.3.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id3.3.3.m1.1b"><apply id="id3.3.3.m1.1.1.cmml" xref="id3.3.3.m1.1.1"><cn id="id3.3.3.m1.1.1.1.cmml" type="integer" xref="id3.3.3.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id3.3.3.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex3"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id4.4.4">Zhiling Yan<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id4.4.4.m1.1"><semantics id="id4.4.4.m1.1a"><msup id="id4.4.4.m1.1.1" xref="id4.4.4.m1.1.1.cmml"><mi id="id4.4.4.m1.1.1a" xref="id4.4.4.m1.1.1.cmml"></mi><mn id="id4.4.4.m1.1.1.1" mathvariant="normal" xref="id4.4.4.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.4.m1.1b"><apply id="id4.4.4.m1.1.1.cmml" xref="id4.4.4.m1.1.1"><cn id="id4.4.4.m1.1.1.1.cmml" type="integer" xref="id4.4.4.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id4.4.4.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex4"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id5.5.5">Chujie Gao<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id5.5.5.m1.1"><semantics id="id5.5.5.m1.1a"><msup id="id5.5.5.m1.1.1" xref="id5.5.5.m1.1.1.cmml"><mi id="id5.5.5.m1.1.1a" xref="id5.5.5.m1.1.1.cmml"></mi><mn id="id5.5.5.m1.1.1.1" mathvariant="normal" xref="id5.5.5.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id5.5.5.m1.1b"><apply id="id5.5.5.m1.1.1.cmml" xref="id5.5.5.m1.1.1"><cn id="id5.5.5.m1.1.1.1.cmml" type="integer" xref="id5.5.5.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id5.5.5.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex5"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex5.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.6">Ruoxi Chen<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id6.6.6.m1.1"><semantics id="id6.6.6.m1.1a"><msup id="id6.6.6.m1.1.1" xref="id6.6.6.m1.1.1.cmml"><mi id="id6.6.6.m1.1.1a" xref="id6.6.6.m1.1.1.cmml"></mi><mn id="id6.6.6.m1.1.1.1" mathvariant="normal" xref="id6.6.6.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id6.6.6.m1.1b"><apply id="id6.6.6.m1.1.1.cmml" xref="id6.6.6.m1.1.1"><cn id="id6.6.6.m1.1.1.1.cmml" type="integer" xref="id6.6.6.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.6.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id6.6.6.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex6"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex6.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id7.7.7">Zhengqing Yuan<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id7.7.7.m1.1"><semantics id="id7.7.7.m1.1a"><msup id="id7.7.7.m1.1.1" xref="id7.7.7.m1.1.1.cmml"><mi id="id7.7.7.m1.1.1a" xref="id7.7.7.m1.1.1.cmml"></mi><mn id="id7.7.7.m1.1.1.1" mathvariant="normal" xref="id7.7.7.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id7.7.7.m1.1b"><apply id="id7.7.7.m1.1.1.cmml" xref="id7.7.7.m1.1.1"><cn id="id7.7.7.m1.1.1.1.cmml" type="integer" xref="id7.7.7.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.7.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id7.7.7.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex7"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex7.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id8.8.8">Yue Huang<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id8.8.8.m1.1"><semantics id="id8.8.8.m1.1a"><msup id="id8.8.8.m1.1.1" xref="id8.8.8.m1.1.1.cmml"><mi id="id8.8.8.m1.1.1a" xref="id8.8.8.m1.1.1.cmml"></mi><mn id="id8.8.8.m1.1.1.1" mathvariant="normal" xref="id8.8.8.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id8.8.8.m1.1b"><apply id="id8.8.8.m1.1.1.cmml" xref="id8.8.8.m1.1.1"><cn id="id8.8.8.m1.1.1.1.cmml" type="integer" xref="id8.8.8.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.8.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id8.8.8.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex8"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex8.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="id9.9.9">Hanchi Sun<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id9.9.9.m1.1"><semantics id="id9.9.9.m1.1a"><msup id="id9.9.9.m1.1.1" xref="id9.9.9.m1.1.1.cmml"><mi id="id9.9.9.m1.1.1a" xref="id9.9.9.m1.1.1.cmml"></mi><mn id="id9.9.9.m1.1.1.1" mathvariant="normal" xref="id9.9.9.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.9.m1.1b"><apply id="id9.9.9.m1.1.1.cmml" xref="id9.9.9.m1.1.1"><cn id="id9.9.9.m1.1.1.1.cmml" type="integer" xref="id9.9.9.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.9.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id9.9.9.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex9"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex9.1.1.1">1</span></span>Equal contributions. The order was determined by rolling dice. Chujie, Ruoxi, Yuan, Yue, and Zhengqing are visiting students in the LAIR lab at Lehigh University. The GitHub link is <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/lichao-sun/SoraReview" title="">https://github.com/lichao-sun/SoraReview</a></span></span></span></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id10.10.10">Jianfeng Gao<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id10.10.10.m1.1"><semantics id="id10.10.10.m1.1a"><msup id="id10.10.10.m1.1.1" xref="id10.10.10.m1.1.1.cmml"><mi id="id10.10.10.m1.1.1a" xref="id10.10.10.m1.1.1.cmml"></mi><mn id="id10.10.10.m1.1.1.1" mathvariant="normal" xref="id10.10.10.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id10.10.10.m1.1b"><apply id="id10.10.10.m1.1.1.cmml" xref="id10.10.10.m1.1.1"><cn id="id10.10.10.m1.1.1.1.cmml" type="integer" xref="id10.10.10.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.10.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id10.10.10.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>  <span class="ltx_text ltx_font_bold" id="id11.11.11">Lifang He<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id11.11.11.m1.1"><semantics id="id11.11.11.m1.1a"><msup id="id11.11.11.m1.1.1" xref="id11.11.11.m1.1.1.cmml"><mi id="id11.11.11.m1.1.1a" xref="id11.11.11.m1.1.1.cmml"></mi><mn id="id11.11.11.m1.1.1.1" mathvariant="normal" xref="id11.11.11.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id11.11.11.m1.1b"><apply id="id11.11.11.m1.1.1.cmml" xref="id11.11.11.m1.1.1"><cn id="id11.11.11.m1.1.1.1.cmml" type="integer" xref="id11.11.11.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.11.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id11.11.11.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>  <span class="ltx_text ltx_font_bold" id="id12.12.12">Lichao Sun<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id12.12.12.m1.1"><semantics id="id12.12.12.m1.1a"><msup id="id12.12.12.m1.1.1" xref="id12.12.12.m1.1.1.cmml"><mi id="id12.12.12.m1.1.1a" xref="id12.12.12.m1.1.1.cmml"></mi><mn id="id12.12.12.m1.1.1.1" mathvariant="normal" xref="id12.12.12.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id12.12.12.m1.1b"><apply id="id12.12.12.m1.1.1.cmml" xref="id12.12.12.m1.1.1"><cn id="id12.12.12.m1.1.1.1.cmml" type="integer" xref="id12.12.12.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.12.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id12.12.12.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnotex10"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex10.1.1.1">2</span></span>Lichao Sun is co-corresponding author: <a class="ltx_ref ltx_href ltx_font_medium" href="mailto:lis221@lehigh.edu" style="color:#000000;" title="">lis221@lehigh.edu</a></span></span></span></span>
<br class="ltx_break"/>
<br class="ltx_break"/><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id13.13.m1.1"><semantics id="id13.13.m1.1a"><msup id="id13.13.m1.1.1" xref="id13.13.m1.1.1.cmml"><mi id="id13.13.m1.1.1a" xref="id13.13.m1.1.1.cmml"></mi><mn id="id13.13.m1.1.1.1" xref="id13.13.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id13.13.m1.1b"><apply id="id13.13.m1.1.1.cmml" xref="id13.13.m1.1.1"><cn id="id13.13.m1.1.1.1.cmml" type="integer" xref="id13.13.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.13.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id13.13.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="id16.15.id1">Lehigh University</span> <math alttext="{}^{2}" class="ltx_Math" display="inline" id="id14.14.m2.1"><semantics id="id14.14.m2.1a"><msup id="id14.14.m2.1.1" xref="id14.14.m2.1.1.cmml"><mi id="id14.14.m2.1.1a" xref="id14.14.m2.1.1.cmml"></mi><mn id="id14.14.m2.1.1.1" xref="id14.14.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id14.14.m2.1b"><apply id="id14.14.m2.1.1.cmml" xref="id14.14.m2.1.1"><cn id="id14.14.m2.1.1.1.cmml" type="integer" xref="id14.14.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.14.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id14.14.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="id17.16.id2">Microsoft Research</span> 
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id18.id1"><span class="ltx_text ltx_font_typewriter" id="id18.id1.1">Sora</span> is a text-to-video generative AI model, released by OpenAI in February 2024.
The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world.
Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model’s background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models.
We first trace <span class="ltx_text ltx_font_typewriter" id="id18.id1.2">Sora</span>’s development and investigate the underlying technologies used to build this “world simulator”.
Then, we describe in detail the applications and potential impact of <span class="ltx_text ltx_font_typewriter" id="id18.id1.3">Sora</span> in multiple industries ranging from film-making and education to marketing.
We discuss the main challenges and limitations that need to be addressed to widely deploy <span class="ltx_text ltx_font_typewriter" id="id18.id1.4">Sora</span>, such as ensuring safe and unbiased video generation.
Lastly, we discuss the future development of <span class="ltx_text ltx_font_typewriter" id="id18.id1.5">Sora</span> and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.

<span class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="267" id="S0.F1.g1" src="extracted/5432776/figures/sora.png" width="377"/>
<span class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_typewriter" id="S0.F1.2.1">Sora</span>: A Breakthrough in AI-Powered Vision Generation.</span>
</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S1" title="1 Introduction ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S2" title="2 Background ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S2.SS1" title="2.1 History ‣ 2 Background ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>History</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S2.SS2" title="2.2 Advanced Concepts ‣ 2 Background ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Advanced Concepts</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3" title="3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Technology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS1" title="3.1 Overview of Sora ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview of <span class="ltx_text ltx_font_typewriter">Sora</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2" title="3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Pre-processing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS1" title="3.2.1 Variable Durations, Resolutions, Aspect Ratios ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Variable Durations, Resolutions, Aspect Ratios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS2" title="3.2.2 Unified Visual Representation ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Unified Visual Representation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS3" title="3.2.3 Video Compression Network ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Video Compression Network</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS4" title="3.2.4 Spacetime Latent Patches ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>Spacetime Latent Patches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS5" title="3.2.5 Discussion ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS6" title="3.2.6 Diffusion Transformer ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.6 </span>Diffusion Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS3" title="3.3 Modeling ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS3.SSS1" title="3.3.1 Discussion ‣ 3.3 Modeling ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4" title="3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Language Instruction Following</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS1" title="3.4.1 Large Language Models ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS2" title="3.4.2 Text-to-Image ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Text-to-Image</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS3" title="3.4.3 Text-to-Video ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Text-to-Video</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS4.SSS4" title="3.4.4 Discussion ‣ 3.4 Language Instruction Following ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5" title="3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Prompt Engineering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS1" title="3.5.1 Text Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Text Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS2" title="3.5.2 Image Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>Image Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS3" title="3.5.3 Video Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.3 </span>Video Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS5.SSS4" title="3.5.4 Discussion ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6" title="3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Trustworthiness</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS1" title="3.6.1 Safety Concern ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>Safety Concern</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS2" title="3.6.2 Other Exploitation ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>Other Exploitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS3" title="3.6.3 Alignment ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.3 </span>Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS6.SSS4" title="3.6.4 Discussion ‣ 3.6 Trustworthiness ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.4 </span>Discussion</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4" title="4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS1" title="4.1 Movie ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Movie</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS2" title="4.2 Education ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Education</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS3" title="4.3 Gaming ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Gaming</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS4" title="4.4 Healthcare ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Healthcare</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.SS5" title="4.5 Robotics ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Robotics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S5" title="5 Discussion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S5.SS1" title="5.1 Limitations ‣ 5 Discussion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S5.SS2" title="5.2 Opportunities ‣ 5 Discussion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Opportunities</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S6" title="6 Conclusion ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#A1" title="Appendix A Related Works ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Related Works</span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Since the release of ChatGPT in November 2022, the advent of AI technologies has marked a significant transformation, reshaping interactions and integrating deeply into various facets of daily life and industry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib2" title="">2</a>]</cite>. Building on this momentum, OpenAI released, in February 2024, <span class="ltx_text ltx_font_typewriter" id="S1.p1.1.1">Sora</span>, a text-to-video generative AI model that can generate videos of realistic or imaginative scenes from text prompts.
Compared to previous video generation models, <span class="ltx_text ltx_font_typewriter" id="S1.p1.1.2">Sora</span> is distinguished by its ability to produce up to 1-minute long videos with high quality while maintaining adherence to user’s text instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>.
This progression of <span class="ltx_text ltx_font_typewriter" id="S1.p1.1.3">Sora</span> is the embodiment of the long-standing AI research mission of equipping AI systems (or AI Agents) with the capability of understanding and interacting with the physical world in motion. This involves developing AI models that are capable of not only interpreting complex user instructions but also applying this understanding to solve real-world problems through dynamic and contextually rich simulations.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="124" id="S1.F2.g1" src="extracted/5432776/figures/sora_in_out.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of <span class="ltx_text ltx_font_typewriter" id="S1.F2.3.1">Sora</span> in text-to-video generation. Text instructions are given to the OpenAI <span class="ltx_text ltx_font_typewriter" id="S1.F2.4.2">Sora</span> model, and it generates three videos according to the instructions.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text ltx_font_typewriter" id="S1.p2.1.1">Sora</span> demonstrates a remarkable ability to accurately interpret and execute complex human instructions, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">2</span></a>. The model can generate detailed scenes that include multiple characters that perform specific actions against intricate backgrounds. Researchers attribute <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.2">Sora</span>’s proficiency to not only processing user-generated textual prompts but also discerning the complicated interplay of elements within a scenario. One of the most striking aspects of <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.3">Sora</span> is its capacity for up to a minute-long video while maintaining high visual quality and compelling visual coherency. Unlike earlier models that can only generate short video clips, <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.4">Sora</span>’s minute-long video creation possesses a sense of progression and a visually consistent journey from its first frame to the last. In addition, <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.5">Sora</span>’s advancements are evident in its ability to produce extended video sequences with nuanced depictions of motion and interaction, overcoming the constraints of shorter clips and simpler visual renderings that characterized earlier video generation models. This capability represents a leap forward in AI-driven creative tools, allowing users to convert text narratives to rich visual stories. Overall, these advances show the potential of <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.6">Sora</span> as a <em class="ltx_emph ltx_font_italic" id="S1.p2.1.7">world simulator</em> to provide nuanced insights into the physical and contextual dynamics of the depicted scenes. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Technology.</span> At the heart of <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.2">Sora</span> is a pre-trained <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">diffusion transformer</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib4" title="">4</a>]</cite>. Transformer models have proven scalable and effective for many natural language tasks. Similar to powerful large language models (LLMs) such as GPT-4, <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.4">Sora</span> can parse text and comprehend complex user instructions. To make video generation computationally efficient, <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.5">Sora</span> employs <em class="ltx_emph ltx_font_italic" id="S1.p3.1.6">spacetime latent patches</em> as its building blocks. Specifically,
<span class="ltx_text ltx_font_typewriter" id="S1.p3.1.7">Sora</span> compresses a raw input video into a latent spacetime representation.
Then, a sequence of latent spacetime patches is extracted from the compressed video to
encapsulate both the visual appearance and motion dynamics over brief intervals.
These patches, analogous to word tokens in language models, provide <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.8">Sora</span> with detailed <em class="ltx_emph ltx_font_italic" id="S1.p3.1.9">visual phrases</em> to be used to construct videos.
<span class="ltx_text ltx_font_typewriter" id="S1.p3.1.10">Sora</span>’s text-to-video generation is performed by a diffusion transformer model. Starting with a frame filled with visual noise, the model iteratively denoises the image and introduces specific details according to the provided text prompt. In essence, the generated video emerges through a multi-step refinement process, with each step refining the video to be more aligned with the desired content and quality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Highlights of <span class="ltx_text ltx_font_typewriter" id="S1.p4.1.1.1">Sora</span>.</span> <span class="ltx_text ltx_font_typewriter" id="S1.p4.1.2">Sora</span>’s capabilities have profound implications in various aspects: </p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">Improving simulation abilities</span>: Training <span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.2">Sora</span> at scale is attributed to its remarkable ability to simulate various aspects of the physical world. Despite lacking explicit 3D modeling, <span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.3">Sora</span> exhibits 3D consistency with dynamic camera motion and long-range coherence that includes object persistence and simulates simple interactions with the world. Moreover, <span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.4">Sora</span> intriguingly simulates digital environments like Minecraft, controlled by a basic policy while maintaining visual fidelity. These emergent abilities suggest that scaling video models is effective in creating AI models to simulate the complexity of physical and digital worlds.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">Boosting creativity</span>:
Imagine outlining a concept through text, whether a simple object or a full scene, and seeing a realistic or highly stylized video rendered within seconds. <span class="ltx_text ltx_font_typewriter" id="S1.I1.i2.p1.1.2">Sora</span> allows an accelerated design process for faster exploration and refinement of ideas, thus significantly boosting the creativity of artists, filmmakers, and designers.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">Driving educational innovations</span>: Visual aids have long been integral to understanding important concepts in education. With <span class="ltx_text ltx_font_typewriter" id="S1.I1.i3.p1.1.2">Sora</span>, educators can easily turn a class plan from text to videos
to captivate students’ attention and improve learning efficiency. From scientific simulations to historical dramatizations, the possibilities are boundless.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.1">Enhancing Accessibility</span>: Enhancing accessibility in the visual domain is paramount. <span class="ltx_text ltx_font_typewriter" id="S1.I1.i4.p1.1.2">Sora</span> offers an innovative solution by converting textual descriptions to visual content. This capability empowers all individuals, including those with visual impairments, to actively engage in content creation and interact with others in more effective ways. Consequently, it allows for a more inclusive environment where everyone has the opportunity to express his or her ideas through videos.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i5.p1.1.1">Fostering emerging applications</span>: The applications of <span class="ltx_text ltx_font_typewriter" id="S1.I1.i5.p1.1.2">Sora</span> are vast. For example, marketers might use it to create dynamic advertisements tailored to specific audience descriptions. Game developers might use it to generate customized visuals or even character actions from player narratives.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Limitations and Opportunities.</span> While <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.2">Sora</span>’s achievements highlight significant advancements in AI, challenges remain. Depicting complex actions or capturing subtle facial expressions are among the areas where the model could be enhanced. In addition, ethical considerations such as mitigating biases in generated content and preventing harmful visual outputs underscore the importance of responsible usage by developers, researchers, and the broader community. Ensuring that <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.3">Sora</span>’s outputs are consistently safe and unbiased is a principal challenge.
The field of video generation is advancing swiftly, with academic and industry research teams making relentless strides. The advent of competing text-to-video models suggests that <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.4">Sora</span> may soon be part of a dynamic ecosystem. This collaborative and competitive environment fosters innovation, leading to improved video quality and new applications that help improve the productivity of workers and make people’s lives more entertaining.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Our Contributions.</span>
Based on published technical reports and our reverse engineering, this paper presents the first comprehensive review of <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.2">Sora</span>’s background, related technologies, emerging applications, current limitations, and future opportunities.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>History</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In the realm of computer vision (CV), prior to the deep learning revolution, traditional image generation techniques relied on methods like texture synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib5" title="">5</a>]</cite> and texture mapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib6" title="">6</a>]</cite>, based on hand-crafted features. However, these methods were limited in their capacity to produce complex and vivid images. The introduction of Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib7" title="">7</a>]</cite> and Variational Autoencoders (VAEs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib8" title="">8</a>]</cite> marked a significant turning point due to its remarkable capabilities across various applications. Subsequent developments, such as flow models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib9" title="">9</a>]</cite> and diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib10" title="">10</a>]</cite>, further enhanced image generation with greater detail and quality. The recent progress in Artificial Intelligence Generated Content (AIGC) technologies has democratized content creation, enabling users to generate desired content through simple textual instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Over the past decade, the development of generative CV models has taken various routes, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S2.F3" title="Figure 3 ‣ 2.1 History ‣ 2 Background ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">3</span></a>. This landscape began to shift notably following the successful application of the transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib12" title="">12</a>]</cite> in NLP, as demonstrated by BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib13" title="">13</a>]</cite> and GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib14" title="">14</a>]</cite>. In CV, researchers take this concept even further by combining the transformer architecture with visual components, allowing it to be applied to downstream CV tasks, such as Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib15" title="">15</a>]</cite> and Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib16" title="">16</a>]</cite>. Parallel to the transformer’s success, diffusion models have also made significant strides in the fields of image and video generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib10" title="">10</a>]</cite>. Diffusion models offer a mathematically sound framework for converting noise into images with U-Nets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib17" title="">17</a>]</cite>, where U-Nets facilitate this process by learning to predict and mitigate noise at each step. Since 2021, a paramount focus in AI has been on generative language and vision models that are capable of interpreting human instructions, known as multimodal models. For example, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib18" title="">18</a>]</cite> is a pioneering vision-language model that combines transformer architecture with visual elements, facilitating its training on vast datasets of text and images. By integrating visual and linguistic knowledge from the outset, CLIP can function as an image encoder within multimodal generation frameworks. Another notable example is Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib19" title="">19</a>]</cite>, a versatile text-to-image AI model celebrated for its adaptability and ease of use. It employs transformer architecture and latent diffusion techniques to decode textual inputs and produce images of a wide array of styles, further illustrating the advancements in multimodal AI.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S2.F3.g1" src="extracted/5432776/figures/history.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>History of Generative AI in Vision Domain.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Following the release of ChatGPT in November 2022, we have witnessed the emergence of commercial text-to-image products in 2023, such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib19" title="">19</a>]</cite>, Midjourney <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib20" title="">20</a>]</cite>, DALL-E 3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib21" title="">21</a>]</cite>. These tools enable users to generate new images of high resolution and quality with simple text prompts, showcasing the potential of AI in creative image generation. However, transitioning from text-to-image to text-to-video is challenging due to the temporal complexity of videos. Despite numerous efforts in industry and academia, most existing video generation tools, such as Pika <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib22" title="">22</a>]</cite> and Gen-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib23" title="">23</a>]</cite>, are limited to producing only short video clips of a few seconds. In this context, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.1">Sora</span> represents a significant breakthrough, akin to ChatGPT’s impact in the NLP domain. <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.2">Sora</span> is the first model that is capable of generating videos up to one minute long based on human instructions, marking a milestone that profoundly influences research and development in generative AI. To facilitate easy access to the latest advancements in vision generation models, the most recent works have been compiled and provided in the Appendix and our GitHub. </p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Advanced Concepts</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">Scaling Laws for Vision Models.</span>   With scaling laws for LLMs, it is natural to ask whether the development of vision models follows similar scaling laws.
Recently, Zhai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib24" title="">24</a>]</cite> have demonstrated that the performance-compute frontier for ViT models with enough training data roughly follows a (saturating) power law. Following them, Google Research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib25" title="">25</a>]</cite> presented a recipe for highly efficient and stable training of a 22B-parameter ViT. Results show that great performance can be achieved using the frozen model to produce embeddings, and then training thin layers on top. <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p1.1.2">Sora</span>, as a large vision model (LVM), aligns with these scaling principles, uncovering several emergent abilities in text-to-video generation. This significant progression underscores the potential for LVMs to achieve advancements like those seen in LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Emergent Abilities.</span>  Emergent abilities in LLMs are sophisticated behaviors or functions that manifest at certain scales—often linked to the size of the model’s parameters—that were not explicitly programmed or anticipated by their developers. These abilities are termed "emergent" because they emerge from the model’s comprehensive training across varied datasets, coupled with its extensive parameter count. This combination enables the model to form connections and draw inferences that surpass mere pattern recognition or rote memorization. Typically, the emergence of these abilities cannot be straightforwardly predicted by extrapolating from the performance of smaller-scale models. While numerous LLMs, such as ChatGPT and GPT-4, exhibit emergent abilities, vision models demonstrating comparable capabilities have been scarce until the advent of Sora. According to <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p2.1.2">Sora</span>’s technical report, it is the first vision model to exhibit confirmed emergent abilities, marking a significant milestone in the field of computer vision.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In addition to its emergent abilities, <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p3.1.1">Sora</span> exhibits other notable capabilities, including instruction following, visual prompt engineering, and video understanding. These aspects of <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p3.1.2">Sora</span>’s functionality represent significant advancements in the vision domain and will be explored and discussed in the rest sections.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Technology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview of <span class="ltx_text ltx_font_typewriter" id="S3.SS1.1.1">Sora</span>
</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="197" id="S3.F4.g1" src="extracted/5432776/fig_kai/sora_framework.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S3.F4.3.1">Reverse Engineering</span>: Overview of <span class="ltx_text ltx_font_typewriter" id="S3.F4.4.2">Sora</span> framework</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In the core essence, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">Sora</span> is a diffusion transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib4" title="">4</a>]</cite> with flexible sampling dimensions as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F4" title="Figure 4 ‣ 3.1 Overview of Sora ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">4</span></a>. It has three parts: (1) A time-space compressor first maps the original video into latent space. (2) A ViT then processes the tokenized latent representation and outputs the denoised latent representation. (3) A CLIP-like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib26" title="">26</a>]</cite> conditioning mechanism receives LLM-augmented user instructions and potentially visual prompts to guide the diffusion model to generate styled or themed videos. After many denoising steps, the latent representation of the generated video is obtained and then mapped back to pixel space with the corresponding decoder. In this section, we aim to reverse engineer the technology used by <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">Sora</span> and discuss a wide range of related works.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Pre-processing</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Variable Durations, Resolutions, Aspect Ratios</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">One distinguishing feature of <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.1.1">Sora</span> is its ability to train on, understand, and generate videos and images at their native sizes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite> as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F5" title="Figure 5 ‣ 3.2.1 Variable Durations, Resolutions, Aspect Ratios ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">5</span></a>. Traditional methods often resize, crop, or adjust the aspect ratios of videos to fit a uniform standard—typically short clips with square frames at fixed low resolutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib27" title="">27</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib28" title="">28</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib29" title="">29</a>]</cite>. Those samples are often generated at a wider temporal stride and rely on separately trained frame-insertion and resolution-rendering models as the final step, creating inconsistency across the video. Utilizing the diffusion transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib4" title="">4</a>]</cite> (see Section <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS4" title="3.2.4 Spacetime Latent Patches ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">3.2.4</span></a>), <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.1.2">Sora</span> is the first model to embrace the diversity of visual data and can sample in a wide array of video and image formats, ranging from widescreen 1920x1080p videos to vertical 1080x1920p videos and everything in between without compromising their original dimensions.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S3.F5.g1" src="extracted/5432776/fig_hanchi/turtle.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_typewriter" id="S3.F5.2.1">Sora</span> can generate images in flexible sizes or resolutions ranging from 1920x1080p to 1080x1920p and anything in between. </figcaption>
</figure>
<figure class="ltx_figure ltx_align_floatright" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="388" id="S3.F6.g1" src="extracted/5432776/fig_hanchi/cropped_training.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A comparison between <span class="ltx_text ltx_font_typewriter" id="S3.F6.2.1">Sora</span> (right) and a modified version of the model (left), which crops videos to square shapes—a common practice in model training—highlights the advantages.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Training on data in their native sizes significantly improves composition and framing in the generated videos. Empirical findings suggest that by maintaining the original aspect ratios, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p2.1.1">Sora</span> achieves a more natural and coherent visual narrative. The comparison between <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p2.1.2">Sora</span> and a model trained on uniformly cropped square videos demonstrates a clear advantage as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F6" title="Figure 6 ‣ 3.2.1 Variable Durations, Resolutions, Aspect Ratios ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">6</span></a>. Videos produced by <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p2.1.3">Sora</span> exhibit better framing, ensuring subjects are fully captured in the scene, as opposed to the sometimes truncated views resulting from square cropping.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">This nuanced understanding and preservation of original video and image characteristics mark a significant advancement in the field of generative models. <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p3.1.1">Sora</span>’s approach not only showcases the potential for more authentic and engaging video generation but also highlights the importance of diversity in training data for achieving high-quality results in generative AI. The training approach of <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p3.1.2">Sora</span> aligns with the core tenet of Richard Sutton’s <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS1.p3.1.3">The Bitter Lesson<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib30" title="">30</a>]</cite></span>, which states that leveraging computation over human-designed features leads to more effective and flexible AI systems. Just as the original design of diffusion transformers seeks simplicity and scalability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib31" title="">31</a>]</cite>, Sora’s strategy of training on data at their native sizes eschews traditional AI reliance on human-derived abstractions, favoring instead a generalist method that scales with computational power. In the rest of this section, we try to reverse engineer the architecture design of <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p3.1.4">Sora</span> and discuss related technologies to achieve this amazing feature.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Unified Visual Representation</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To effectively process diverse visual inputs including images and videos with varying durations, resolutions, and aspect ratios, a crucial approach involves transforming all forms of visual data into a unified representation, which facilitates the large-scale training of generative models. Specifically, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS2.p1.1.1">Sora</span> patchifies videos by initially compressing videos into a lower-dimensional latent space, followed by decomposing the representation into spacetime patches. However, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS2.p1.1.2">Sora</span>’s technical report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite> merely presents a high-level idea, making reproduction challenging for the research community. In this section, we try to reverse-engineer the potential ingredients and technical pathways. Additionally, we will discuss viable alternatives that could replicate <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS2.p1.1.3">Sora</span>’s functionalities, drawing upon insights from existing literature.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="110" id="S3.F7.g1" src="extracted/5432776/fig_kai/sora.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>At a high level, <span class="ltx_text ltx_font_typewriter" id="S3.F7.3.1">Sora</span> turns videos into patches by first compressing videos into a lower-dimensional latent space, and subsequently decomposing the representation into spacetime patches. Source: <span class="ltx_text ltx_font_typewriter" id="S3.F7.4.2">Sora</span>’s technical report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Video Compression Network</h4>
<figure class="ltx_figure ltx_align_floatright" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S3.F8.g1" src="extracted/5432776/fig_kai/patchify.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>ViT splits an image into fixed-size patches, linearly embeds each of them, adds position embeddings, and feeds the resulting sequence of vectors to a standard Transformer encoder.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.p1.1.1">Sora</span>’s video compression network (or visual encoder) aims to reduce the dimensionality of input data, especially a raw video, and output a latent representation that is compressed both temporally and spatially as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F7" title="Figure 7 ‣ 3.2.2 Unified Visual Representation ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">7</span></a>. According to the references in the technical report, the compression network is built upon VAE or Vector Quantised-VAE (VQ-VAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib32" title="">32</a>]</cite>. However, it is challenging for VAE to map visual data of any size to a unified and fixed-sized latent space if resizing and cropping are not used as mentioned in the technical report. We summarize two distinct implementations
to address this issue:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p2.1.1">Spatial-patch Compression.</span>  This involves transforming video frames into fixed-size patches, akin to the methodologies employed in ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib15" title="">15</a>]</cite> and MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib33" title="">33</a>]</cite> (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F8" title="Figure 8 ‣ 3.2.3 Video Compression Network ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">8</span></a>), before encoding them into a latent space. This approach is particularly effective for accommodating videos of varying resolutions and aspect ratios, as it encodes entire frames through the processing of individual patches. Subsequently, these spatial tokens are organized in a temporal sequence to create a spatial-temporal latent representation. This technique highlights several critical considerations: <span class="ltx_text ltx_font_italic ltx_framed_underline" id="S3.SS2.SSS3.p2.1.2">Temporal dimension variability</span> – given the varying durations of training videos, the temporal dimension of the latent space representation cannot be fixed. To address this, one can either sample a specific number of frames (padding or temporal interpolation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib34" title="">34</a>]</cite> may be needed for much shorter videos) or define a universally extended (super long) input length for subsequent processing (more details are described in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS4" title="3.2.4 Spacetime Latent Patches ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">3.2.4</span></a>); <span class="ltx_text ltx_font_italic ltx_framed_underline" id="S3.SS2.SSS3.p2.1.3">Utilization of pre-trained visual encoders</span> – for processing videos of high resolution, leveraging existing pre-trained visual encoders, such as the VAE encoder from Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib19" title="">19</a>]</cite>, is advisable for most researchers while <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.p2.1.4">Sora</span>’s team is expected to train their own compression network with a decoder (the video generator) from scratch via the manner employed in training latent diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib36" title="">36</a>]</cite>. These encoders can efficiently compress large-size patches (e.g., <math alttext="256\times 256" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.1.m1.1"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mrow id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml"><mn id="S3.SS2.SSS3.p2.1.m1.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml">256</mn><mo id="S3.SS2.SSS3.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS3.p2.1.m1.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><apply id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1"><times id="S3.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.1"></times><cn id="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS3.p2.1.m1.1.1.2">256</cn><cn id="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS3.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.1.m1.1d">256 × 256</annotation></semantics></math>), facilitating the management of large-scale data; <span class="ltx_text ltx_font_italic ltx_framed_underline" id="S3.SS2.SSS3.p2.1.5">Temporal information aggregation</span> – since this method primarily focuses on spatial patch compression, it necessitates an additional mechanism for aggregating temporal information within the model. This aspect is crucial for capturing dynamic changes over time and is further elaborated in subsequent sections (see details in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS6" title="3.2.6 Diffusion Transformer ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">3.2.6</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F14" title="Figure 14 ‣ 3.3 Modeling ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">14</span></a>).
</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p3.1.1">Spatial-temporal-patch Compression.</span>  This technique is designed to encapsulate both spatial and temporal dimensions of video data, offering a comprehensive representation. This technique extends beyond merely analyzing static frames by considering the movement and changes across frames, thereby capturing the video’s dynamic aspects. The utilization of 3D convolution emerges as a straightforward and potent method for achieving this integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib37" title="">37</a>]</cite>. The graphical illustration and the comparison against pure spatial-pachifying are depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F9" title="Figure 9 ‣ 3.2.3 Video Compression Network ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">9</span></a>. Similar to spatial-patch compression, employing spatial-temporal-patch compression with predetermined convolution kernel parameters – such as fixed kernel sizes, strides, and output channels – results in variations in the dimensions of the latent space due to the differing characteristics of video inputs. This variability is primarily driven by the diverse durations and resolutions of the videos being processed. To mitigate this challenge, the approaches adopted for spatial patchification are equally applicable and effective in this context.</p>
</div>
<figure class="ltx_figure" id="S3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_center ltx_align_middle" id="S3.F9.1" style="width:256.1pt;">
<span class="ltx_para ltx_align_center" id="S3.F9.1.p1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="126" id="S3.F9.1.p1.g1" src="extracted/5432776/fig_kai/2d_st_patch.png" width="348"/>
</span></span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_center ltx_align_middle" id="S3.F9.2" style="width:199.2pt;">
<span class="ltx_para ltx_align_center" id="S3.F9.2.p1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="157" id="S3.F9.2.p1.g1" src="extracted/5432776/fig_kai/3d_st_patch.png" width="261"/>
</span></span></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Comparison between different patchification for video compression. Source: ViViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib38" title="">38</a>]</cite>. (<span class="ltx_text ltx_font_bold" id="S3.F9.7.1">Left</span>) Spatial patchification simply samples <math alttext="n_{t}" class="ltx_Math" display="inline" id="S3.F9.4.m1.1"><semantics id="S3.F9.4.m1.1b"><msub id="S3.F9.4.m1.1.1" xref="S3.F9.4.m1.1.1.cmml"><mi id="S3.F9.4.m1.1.1.2" xref="S3.F9.4.m1.1.1.2.cmml">n</mi><mi id="S3.F9.4.m1.1.1.3" xref="S3.F9.4.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F9.4.m1.1c"><apply id="S3.F9.4.m1.1.1.cmml" xref="S3.F9.4.m1.1.1"><csymbol cd="ambiguous" id="S3.F9.4.m1.1.1.1.cmml" xref="S3.F9.4.m1.1.1">subscript</csymbol><ci id="S3.F9.4.m1.1.1.2.cmml" xref="S3.F9.4.m1.1.1.2">𝑛</ci><ci id="S3.F9.4.m1.1.1.3.cmml" xref="S3.F9.4.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F9.4.m1.1d">n_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.F9.4.m1.1e">italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> frames and embeds each 2D frame independently following ViT. (<span class="ltx_text ltx_font_bold" id="S3.F9.8.2">Right</span>) Spatial-temporal patchification extracts and linearly embeds non-overlapping or overlapping tubelets that span the spatiotemporal input volume.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS3.p4">
<p class="ltx_p" id="S3.SS2.SSS3.p4.1">In summary, we reverse engineer the two patch-level compression approaches based on VAE or its variant like VQ-VQE because operations on patches are more flexible to process different types of videos. Since <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.p4.1.1">Sora</span> aims to generate high-fidelity videos, a large patch size or kernel size is used for efficient compression. Here, we expect that fixed-size patches are used for simplicity, scalability, and training stability. But varying-size patches could also be used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib39" title="">39</a>]</cite> to make the dimension of the whole frames or videos in latent space consistent. However, it may result in invalid positional encoding, and cause challenges for the decoder to generate videos with varying-size latent patches.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Spacetime Latent Patches</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">There is a pivotal concern remaining in the compression network part: How to handle the variability in latent space dimensions (i.e., the number of latent feature chunks or patches from different video types) before feeding patches into the input layers of the diffusion transformer. Here, we discuss several solutions.
</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS4.p2">
<p class="ltx_p" id="S3.SS2.SSS4.p2.1">Based on <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS4.p2.1.1">Sora</span>’s technical report and the corresponding references, <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS4.p2.1.2">patch n’ pack (PNP)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib40" title="">40</a>]</cite> is likely the solution. PNP packs multiple patches from different images in a single sequence as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F10" title="Figure 10 ‣ 3.2.4 Spacetime Latent Patches ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">10</span></a>. This method is inspired by example packing used in natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib41" title="">41</a>]</cite> that accommodates efficient training on variable length inputs by dropping tokens. Here the patchification and token embedding steps need to be completed in the compression network, but <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS4.p2.1.3">Sora</span> may further patchify the latent for transformer token as Diffusion Transformer does <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib4" title="">4</a>]</cite>.
Regardless there is a second-round patchification or not,
we need to address two concerns, how to pack those tokens in a compact manner and how to control which tokens should be dropped. For the first concern, a simple greedy approach is used which adds examples to the first sequence with enough remaining space. Once no more example can fit, sequences are filled with padding tokens, yielding the fixed sequence lengths needed for batched operations. Such a simple packing algorithm can lead to significant padding, depending on the distribution of the length of inputs. On the other hand, we can control the resolutions and frames we sample to ensure efficient packing by tuning the sequence length and limiting padding. For the second concern, an intuitive approach is to drop the similar tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib44" title="">44</a>]</cite> or, like PNP, apply dropping rate schedulers. However, it is worth noting that <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS4.p2.1.4">3D Consistency</span> is one of the nice properties of <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS4.p2.1.5">Sora</span>. Dropping tokens may ignore fine-grained details during training. Thus, we believe that OpenAI is likely to use a super long context window and pack all tokens from videos although doing so is computationally expensive e.g., the multi-head attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib46" title="">46</a>]</cite> operator exhibits quadratic cost in sequence length.
Specifically, spacetime latent patches from a long-duration video can be packed in one sequence while the ones from several short-duration videos are concatenated in the other sequence.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="155" id="S3.F10.g1" src="extracted/5432776/fig_kai/pnp_seq2.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Patch packing enables variable resolution images or videos with preserved aspect ratio.6 Token dropping somehow could be treated as data augmentation. Source: NaViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib40" title="">40</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>Discussion</h4>
<div class="ltx_para" id="S3.SS2.SSS5.p1">
<p class="ltx_p" id="S3.SS2.SSS5.p1.1">We discuss two technical solutions to data pre-processing that <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS5.p1.1.1">Sora</span> may use. Both solutions are performed at the patch level due to the characteristics of flexibility and scalability for modeling. Different from previous approaches where videos are resized, cropped, or trimmed to a standard size, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS5.p1.1.2">Sora</span> trains on data at its native size. Although there are several benefits (see detailed analysis in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS1" title="3.2.1 Variable Durations, Resolutions, Aspect Ratios ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>), it brings some technical challenges, among which one of the most significant is that neural networks cannot inherently process visual data of variable durations, resolutions, and aspect ratios. Through reverse engineering, we believe that <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS5.p1.1.3">Sora</span> firstly compresses visual patches into low-dimensional latent representations, and arranges such latent patches or further patchified latent patches in a sequence, then injects noise into these latent patches before feeding them to the input layer of diffusion transformer. Spatial-temporal patchification is adopted by <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS5.p1.1.4">Sora</span> because it is simple to implement, and it can effectively reduce the context length with high-information-density tokens and decrease the complexity of subsequent modeling of temporal information.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS5.p2">
<p class="ltx_p" id="S3.SS2.SSS5.p2.1">To the research community, we recommend using cost-efficient alternative solutions for video compression and representation, including utilizing pre-trained checkpoints (e.g., compression network) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib47" title="">47</a>]</cite>, shortening the context window, using light-weight modeling mechanisms such as (grouped) multi-query attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib49" title="">49</a>]</cite> or efficient architectures (e.g. Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib50" title="">50</a>]</cite>), downsampling data and dropping tokens if necessary. The trade-off between effectiveness and efficiency for video modeling is an important research topic to be explored.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.6 </span>Diffusion Transformer</h4>
<figure class="ltx_figure" id="S3.F11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The overall framework of DiT (left) and U-ViT (right)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S3.F11.g1" src="extracted/5432776/fig_yx/dit-uvit.png" width="568"/></div>
<div class="ltx_flex_cell">
<p class="ltx_p ltx_align_center" id="S3.F11.1">.
</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The overall framework of DiT (left) and U-ViT (right)</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Modeling</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Image Diffusion Transformer.</span> Traditional diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib53" title="">53</a>]</cite> mainly leverage convolutional U-Nets that include downsampling and upsampling blocks for the denoising network backbone. However, recent studies show that the U-Net architecture is not crucial to the good performance of the diffusion model. By incorporating a more flexible transformer architecture, the transformer-based diffusion models can use more training data and larger model parameters. Along this line,
DiT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib4" title="">4</a>]</cite> and U-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib54" title="">54</a>]</cite> are among the first works to employ vision transformers for latent diffusion models.
As in ViT, DiT employs a multi-head self-attention layer and a pointwise feed-forward network interlaced with some layer norm and scaling layers. Moreover, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F11" title="Figure 11 ‣ 3.2.6 Diffusion Transformer ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">11</span></a>, DiT incorporates conditioning via adaptive layer norm (AdaLN) with an additional MLP layer for zero-initializing, which initializes each residual block as an identity function and thus greatly stabilizes the training process. The scalability and flexibility of DiT is empirically validated. DiT becomes the new backbone for diffusion models.
In U-ViT, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F11" title="Figure 11 ‣ 3.2.6 Diffusion Transformer ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">11</span></a>, they treat all inputs, including time, condition, and noisy image patches, as tokens and propose long skip connections between the shallow and deep transformer layers. The results suggest that the downsampling and upsampling operators in CNN-based U-Net are not always necessary, and U-ViT achieves record-breaking FID scores in image and text-to-image generation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Like Masked AutoEncoder (MAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib33" title="">33</a>]</cite>,
Masked Diffusion Transformer (MDT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib55" title="">55</a>]</cite> incorporates mask latent modeling into the diffusion process to explicitly enhance contextual relation learning among object semantic parts in image synthesis. Specifically, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F12" title="Figure 12 ‣ 3.3 Modeling ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">12</span></a>, MDT uses a side-interpolated for an additional masked token reconstruction task during training to boost the training efficiency and learn powerful context-aware positional embedding for inference. Compared to DiT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib4" title="">4</a>]</cite>, MDT achieves better performance and faster learning speed. Instead of using AdaLN (i.e., shifting and scaling) for time-conditioning modeling, Hatamizadeh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib56" title="">56</a>]</cite> introduce Diffusion Vision Transformers (DiffiT), which uses a time-dependent self-attention (TMSA) module to model dynamic denoising behavior over sampling time steps. Besides, DiffiT uses two hybrid hierarchical architectures for efficient denoising in the pixel space and the latent space, respectively, and achieves new state-of-the-art results across various generation tasks. Overall, these studies show promising results in employing vision transformers for image latent diffusion, paving the way for future studies for other modalities.</p>
</div>
<figure class="ltx_figure" id="S3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="193" id="S3.F12.g1" src="extracted/5432776/fig_yx/mdt.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>The overall framework of Masked Diffusion Transformer (MDT). A solid/dotted line indicates the training/inference process for each time step. Masking and side-interpolater are only used during training and are removed during inference.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Video Diffusion Transformer.</span> Building upon the foundational works in text-to-image (T2I) diffusion models, recent research has been focused on realizing the potential of diffusion transformers for text-to-video (T2V) generation tasks. Due to the temporal nature of videos, key challenges for applying DiTs in the video domain are: <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.2">i) how to compress the video spatially and temporally to a latent space for efficient denoising</span>; <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.3">ii) how to convert the compressed latent to patches and feed them to the transformer;</span> and <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.4">iii) how to handle long-range temporal and spatial dependencies and ensure content consistency</span>. Please refer to Section <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.SS2.SSS3" title="3.2.3 Video Compression Network ‣ 3.2 Data Pre-processing ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">3.2.3</span></a> for the first challenge. In this Section, we focus our discussion on transformer-based denoising network architectures designed to operate in the spatially and temporally compressed latent space. We give a detailed review of the two important works (Imagen Video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib29" title="">29</a>]</cite> and Video LDM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib36" title="">36</a>]</cite>) described in the reference list of the OpenAI <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.5">Sora</span> technique report.
</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Imagen Video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib29" title="">29</a>]</cite>, a text-to-video generation system developed by Google Research, utilizes a cascade of diffusion models, which consists of 7 sub-models that perform text-conditional video generation, spatial super-resolution, and temporal super-resolution, to transform textual prompts into high-definition videos. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F13" title="Figure 13 ‣ 3.3 Modeling ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">13</span></a>, firstly, a frozen T5 text encoder generates contextual embeddings from the input text prompt. These embeddings are critical for aligning the generated video with the text prompt and are injected into all models in the cascade, in addition to the base model. Subsequently, the embedding is fed to the base model for low-resolution video generation, which is then refined by cascaded diffusion models to increase the resolution. The base video and super-resolution models use a 3D U-Net architecture in a space-time separable fashion. This architecture weaves temporal attention and convolution layers with spatial counterparts to efficiently capture inter-frame dependencies. It employs v-prediction parameterization for numerical stability and conditioning augmentation to facilitate parallel training across models. The process involves joint training on both images and videos, treating each image as a frame to leverage larger datasets, and using classifier-free guidance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib57" title="">57</a>]</cite> to enhance prompt fidelity. Progressive distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib58" title="">58</a>]</cite> is applied to streamline the sampling process, significantly reducing the computational load while maintaining perceptual quality. Combining these methods and techniques allows Imagen Video to generate videos with not only high fidelity but also remarkable controllability, as demonstrated by its ability to produce diverse videos, text animations, and content in various artistic styles.</p>
</div>
<figure class="ltx_figure" id="S3.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="152" id="S3.F13.g1" src="extracted/5432776/fig_yx/imagenV.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>The overall framework of Imagen Video. Source: Imagen Video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib29" title="">29</a>]</cite>.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F14.sf1"><span class="ltx_inline-para ltx_minipage ltx_align_middle" id="S3.F14.sf1.1" style="width:203.8pt;">
<span class="ltx_para ltx_align_center" id="S3.F14.sf1.1.p1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="467" id="S3.F14.sf1.1.p1.g1" src="x1.png" width="830"/>
</span></span>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F14.sf1.10.4.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text ltx_font_bold" id="S3.F14.sf1.7.3" style="font-size:80%;">Additional temporal layer<span class="ltx_text ltx_font_medium" id="S3.F14.sf1.7.3.3">. A pre-trained LDM is turned into a video generator by inserting temporal layers that learn to align frames into temporally consistent sequences. During optimization, the image backbone <math alttext="\theta" class="ltx_Math" display="inline" id="S3.F14.sf1.5.1.1.m1.1"><semantics id="S3.F14.sf1.5.1.1.m1.1b"><mi id="S3.F14.sf1.5.1.1.m1.1.1" xref="S3.F14.sf1.5.1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.F14.sf1.5.1.1.m1.1c"><ci id="S3.F14.sf1.5.1.1.m1.1.1.cmml" xref="S3.F14.sf1.5.1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F14.sf1.5.1.1.m1.1d">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.F14.sf1.5.1.1.m1.1e">italic_θ</annotation></semantics></math> remains fixed and only the parameters <math alttext="\phi" class="ltx_Math" display="inline" id="S3.F14.sf1.6.2.2.m2.1"><semantics id="S3.F14.sf1.6.2.2.m2.1b"><mi id="S3.F14.sf1.6.2.2.m2.1.1" xref="S3.F14.sf1.6.2.2.m2.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.F14.sf1.6.2.2.m2.1c"><ci id="S3.F14.sf1.6.2.2.m2.1.1.cmml" xref="S3.F14.sf1.6.2.2.m2.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F14.sf1.6.2.2.m2.1d">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.F14.sf1.6.2.2.m2.1e">italic_ϕ</annotation></semantics></math> of the temporal layers <math alttext="l_{\phi}^{i}" class="ltx_Math" display="inline" id="S3.F14.sf1.7.3.3.m3.1"><semantics id="S3.F14.sf1.7.3.3.m3.1b"><msubsup id="S3.F14.sf1.7.3.3.m3.1.1" xref="S3.F14.sf1.7.3.3.m3.1.1.cmml"><mi id="S3.F14.sf1.7.3.3.m3.1.1.2.2" xref="S3.F14.sf1.7.3.3.m3.1.1.2.2.cmml">l</mi><mi id="S3.F14.sf1.7.3.3.m3.1.1.2.3" xref="S3.F14.sf1.7.3.3.m3.1.1.2.3.cmml">ϕ</mi><mi id="S3.F14.sf1.7.3.3.m3.1.1.3" xref="S3.F14.sf1.7.3.3.m3.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.F14.sf1.7.3.3.m3.1c"><apply id="S3.F14.sf1.7.3.3.m3.1.1.cmml" xref="S3.F14.sf1.7.3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F14.sf1.7.3.3.m3.1.1.1.cmml" xref="S3.F14.sf1.7.3.3.m3.1.1">superscript</csymbol><apply id="S3.F14.sf1.7.3.3.m3.1.1.2.cmml" xref="S3.F14.sf1.7.3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F14.sf1.7.3.3.m3.1.1.2.1.cmml" xref="S3.F14.sf1.7.3.3.m3.1.1">subscript</csymbol><ci id="S3.F14.sf1.7.3.3.m3.1.1.2.2.cmml" xref="S3.F14.sf1.7.3.3.m3.1.1.2.2">𝑙</ci><ci id="S3.F14.sf1.7.3.3.m3.1.1.2.3.cmml" xref="S3.F14.sf1.7.3.3.m3.1.1.2.3">italic-ϕ</ci></apply><ci id="S3.F14.sf1.7.3.3.m3.1.1.3.cmml" xref="S3.F14.sf1.7.3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F14.sf1.7.3.3.m3.1d">l_{\phi}^{i}</annotation><annotation encoding="application/x-llamapun" id="S3.F14.sf1.7.3.3.m3.1e">italic_l start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> are trained.
</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F14.sf2"><span class="ltx_inline-para ltx_minipage ltx_align_middle" id="S3.F14.sf2.1" style="width:203.8pt;">
<span class="ltx_para ltx_align_center" id="S3.F14.sf2.1.p1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="528" id="S3.F14.sf2.1.p1.g1" src="x2.png" width="830"/>
</span></span>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F14.sf2.4.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text ltx_font_bold" id="S3.F14.sf2.5.2" style="font-size:80%;">Video LDM stack.<span class="ltx_text ltx_font_medium" id="S3.F14.sf2.5.2.1"> Video LDM first generates sparse key frames and then temporally interpolates twice with the same latent diffusion models to achieve a high frame rate. Finally, the latent video is decoded to pixel space, and optionally, a video upsampler diffusion model is applied.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>The overall framework of Video LDM. Source: Video LDM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib36" title="">36</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">Blattmann et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib36" title="">36</a>]</cite> propose to turn a 2D Latent Diffusion Model into a Video Latent Diffusion Model (Video LDM). They achieve this by adding some post-hoc temporal layers among the existing spatial layers into both the U-Net backbone and the VAE decoder that learns to align individual frames. These temporal layers are trained on encoded video data, while the spatial layers remain fixed, allowing the model to leverage large image datasets for pre-training. The LDM’s decoder is fine-tuned for temporal consistency in pixel space and temporally aligning diffusion model upsamplers for enhanced spatial resolution. To generate very long videos, models are trained to predict a future frame given a number of context frames, allowing for classifier-free guidance during sampling.
To achieve high temporal resolution, the video synthesis process is divided into key frame generation and interpolation between these key frames. Following cascaded LDMs, a DM is used to further scale up the Video LDM outputs by 4 times, ensuring high spatial resolution while maintaining temporal consistency. This approach enables the generation of globally coherent long videos in a computationally efficient manner. Additionally, the authors demonstrate the ability to transform pre-trained image LDMs (e.g., Stable Diffusion) into text-to-video models by training only the temporal alignment layers, achieving video synthesis with resolutions up to 1280 × 2048.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Discussion</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.3"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p1.3.1">Cascade diffusion models for spatial and temporal up-sampling. </span><span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS1.p1.3.2">Sora</span> can generate high-resolution videos. By reviewing existing works and our reverse engineering, we speculate that <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS1.p1.3.3">Sora</span> also leverages cascade diffusion model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib59" title="">59</a>]</cite> which is composed of a base model and many space-time refiner models.
The attention modules are unlikely to be heavily used in the based diffusion model and low-resolution diffusion model, considering the high computation cost and limited performance gain of using attention machines in high-resolution cases.
For spatial and temporal scene consistency, as previous works show that temporal consistency is more important than spatial consistency for video/scene generation, <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS1.p1.3.4">Sora</span> is likely to leverage an efficient training strategy by using longer video (for temporal consistency) with lower resolution.
Moreover, <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS1.p1.3.5">Sora</span> is likely to use a <math alttext="v" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mi id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><ci id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">italic_v</annotation></semantics></math>-parameterization diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib58" title="">58</a>]</cite>, considering its superior performance compared to other variants that predict the original latent <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.2.m2.1"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mi id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><ci id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.2.m2.1d">italic_x</annotation></semantics></math> or the noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.3.m3.1"><semantics id="S3.SS3.SSS1.p1.3.m3.1a"><mi id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.1b"><ci id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.3.m3.1d">italic_ϵ</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.1">On the latent encoder. </span>For training efficiency, most of the existing works leverage the pre-trained VAE encoder of Stable Diffusions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib61" title="">61</a>]</cite>, a pre-trained 2D diffusion model, as an initialized model checkpoint. However, the encoder lacks the temporal compression ability. Even though some works propose to only fine-tune the decoder for handling temporal information, the decoder’s performance of dealing with video temporal data in the compressed latent space remains sub-optimal. Based on the technique report, our reverse engineering shows that, instead of using an existing pre-trained VAE encoder, it is likely that <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS1.p2.1.2">Sora</span> uses a space-time VAE encoder, trained from scratch on video data, which performs better than existing ones with a video-orient compressed latent space.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Language Instruction Following</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Users primarily engage with generative AI models through natural language instructions, known as text prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib63" title="">63</a>]</cite>.
Model instruction tuning aims to enhance AI models’ capability to follow prompts accurately. This improved capability in prompt following enables models to generate output that more closely resembles human responses to natural language queries.
We start our discussion with a review of instruction following techniques for large language models (LLMs) and text-to-image models such as DALL·E 3.
To enhance the text-to-video model’s ability to follow text instructions, <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.1">Sora</span> utilizes an approach similar to that of DALL·E 3. The approach involves training a descriptive captioner and utilizing the captioner’s generated data for fine-tuning.
As a result of instruction tuning, <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.2">Sora</span> is able to accommodate a wide range of user requests, ensuring meticulous attention to the details in the instructions and generating videos that precisely meet users’ needs.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Large Language Models</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">The capability of LLMs to follow instructions has been extensively explored <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib66" title="">66</a>]</cite>. This ability allows LLMs to read, understand, and respond appropriately to instructions describing an unseen task without examples. Prompt following ability is obtained and enhanced by fine-tuning LLMs on a mixture of tasks formatted as instructions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib66" title="">66</a>]</cite>, known as instruction tuning. Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib65" title="">65</a>]</cite> showed that instruction-tuned LLMs significantly outperform the untuned ones on unseen tasks. The instruction-following ability transforms LLMs into general-purpose task solvers, marking a paradigm shift in the history of AI development. </p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Text-to-Image</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">The instruction following in DALL·E 3 is addressed by a caption improvement method with a hypothesis that the quality of text-image pairs that the model is trained on determines the performance of the resultant text-to-image model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib67" title="">67</a>]</cite>. The poor quality of data, particularly the prevalence of noisy data and short captions that omit a large amount of visual information, leads to many issues such as neglecting keywords and word order, and misunderstanding the user intentions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib21" title="">21</a>]</cite>. The caption improvement approach addresses these issues by re-captioning existing images with detailed, descriptive captions. The approach first trains an image captioner, which is a vision-language model, to generate precise and descriptive image captions. The resulting descriptive image captions by the captioner are then used to fine-tune text-to-image models. Specifically, DALL·E 3 follows contrastive captioners (CoCa) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib68" title="">68</a>]</cite> to jointly train an image captioner with a CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib26" title="">26</a>]</cite> architecture and a language model objective.
This image captioner incorporates an image encoder a unimodal text encoder for extracting language information, and a multimodal text decoder. It first employs a contrastive loss between unimodal image and text embeddings, followed by a captioning loss for the multimodal decoder’s outputs. The resulting image captioner is further fine-tuned on a highly detailed description of images covering main objects, surroundings, backgrounds, texts, styles, and colorations. With this step, the image captioner is able to generate detailed descriptive captions for the images. The training dataset for the text-to-image model is a mixture of the re-captioned dataset generated by the image captioner and ground-truth human-written data to ensure that the model captures user inputs. This image caption improvement method introduces a potential issue: a mismatch between the actual user prompts and descriptive image descriptions from the training data. DALL·E 3 addresses this by <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p1.1.1">upsampling</em>, where LLMs are used to re-write short user prompts into detailed and lengthy instructions. This ensures that the model’s text inputs received in inference time are consistent with those in model training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Text-to-Video</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">To enhance the ability of instruction following, <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.1">Sora</span> adopts a similar caption improvement approach. This method is achieved by first training a video captioner capable of producing detailed descriptions for videos. Then, this video captioner is applied to all videos in the training data to generate high-quality (video, descriptive caption) pairs, which are used to fine-tune <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p1.1.2">Sora</span> to improve its instruction following ability.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.1"><span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p2.1.1">Sora</span>’s technical report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite> does not reveal the details about how the video captioner is trained. Given that the video captioner is a video-to-text model, there are many approaches to building it. A straightforward approach is to utilize CoCa architecture for video captioning by taking multiple frames of a video and feeding each frame into the image encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib68" title="">68</a>]</cite>, known as VideoCoCa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib69" title="">69</a>]</cite>. VideoCoCa builds upon CoCa and re-uses the image encoder pre-trained weights and applies it independently on sampled video frames. The resulting frame token embeddings are flattened and concatenated into a long sequence of video representations. These flattened frame tokens are then processed by a generative pooler and a contrastive pooler, which are jointly trained with the contrastive loss and captioning loss. Other alternatives to building video captioners include mPLUG-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib70" title="">70</a>]</cite>, GIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib71" title="">71</a>]</cite>, FrozenBiLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib72" title="">72</a>]</cite>, and more. Finally, to ensure that user prompts align with the format of those descriptive captions in training data, <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS3.p2.1.2">Sora</span> performs an additional prompt extension step, where GPT-4V is used to expand user inputs to detailed descriptive prompts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>Discussion</h4>
<div class="ltx_para" id="S3.SS4.SSS4.p1">
<p class="ltx_p" id="S3.SS4.SSS4.p1.1">The instruction-following ability is critical for <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS4.p1.1.1">Sora</span> to generate one-minute-long videos with intricate scenes that are faithful to user intents. According to <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS4.p1.1.2">Sora</span>’s technical report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>, this ability is obtained by developing a captioner that can generate long and detailed captions, which are then used to train the model. However, the process of collecting data for training such a captioner is unknown and likely labor-intensive, as it may require detailed descriptions of videos. Moreover, the descriptive video captioner might hallucinate important details of the videos. We believe that how to improve the video captioner warrants further investigation and is critical to enhance the instruction-following ability of text-to-image models.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Prompt Engineering</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Prompt engineering refers to the process of designing and refining the input given to an AI system, particularly in the context of generative models, to achieve specific or optimized outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib75" title="">75</a>]</cite>. The art and science of prompt engineering involve crafting these inputs in a way that guides the model to produce the most accurate, relevant, and coherent responses possible.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Text Prompt</h4>
<div class="ltx_para" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.1">Text prompt engineering is vital in directing text-to-video models (e.g., <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS1.p1.1.1">Sora</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>) to produce videos that are visually striking while precisely meeting user specifications. This involves crafting detailed descriptions
to instruct the model to effectively bridge the gap between human creativity and AI’s execution capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib76" title="">76</a>]</cite>. The prompts for <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS1.p1.1.2">Sora</span> cover a wide range of scenarios. Recent works (e.g., VoP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib77" title="">77</a>]</cite>, Make-A-Video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib28" title="">28</a>]</cite>, and Tune-A-Video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib78" title="">78</a>]</cite>) have shown how prompt engineering leverages model’s natural language understanding ability to decode complex instructions and render them into cohesive, lively, and high-quality video narratives. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F15" title="Figure 15 ‣ 3.5.1 Text Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">15</span></a>, “a stylish woman walking down a neon-lit Tokyo street…” is such a meticulously crafted text prompt that it ensures <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS1.p1.1.3">Sora</span> to generate a video that aligns well with the expected vision. The quality of prompt engineering depends on the careful selection of words, the specificity of the details provided, and comprehension of their impact on the model’s output. For example, the prompt in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F15" title="Figure 15 ‣ 3.5.1 Text Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">15</span></a> specifies in detail the actions, settings, character appearances, and even the desired mood and atmosphere of the scene.
</p>
</div>
<figure class="ltx_figure" id="S3.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S3.F15.g1" src="extracted/5432776/figures/text_prompt.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>A case study on prompt engineering for text-to-video generation, employing color coding to delineate the creative process. The text highlighted in blue describes the elements generated by <span class="ltx_text ltx_font_typewriter" id="S3.F15.2.1">Sora</span>, such as the depiction of a stylish woman. In contrast, the text in yellow accentuates the model’s interpretation of actions, settings, and character appearances, demonstrating how a meticulously crafted prompt is transformed into a vivid and dynamic video narrative.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>Image Prompt</h4>
<div class="ltx_para" id="S3.SS5.SSS2.p1">
<p class="ltx_p" id="S3.SS5.SSS2.p1.1">An image prompt serves as a visual anchor for the to-be-generated video’s content and other elements such as
characters, setting, and mood <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib79" title="">79</a>]</cite>.
In addition, a text prompt can instruct the model to animate these elements by e.g., adding layers of movement, interaction, and narrative progression that bring the static image to life  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib81" title="">81</a>]</cite>.
The use of image prompts allows <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS2.p1.1.1">Sora</span> to convert static images into dynamic, narrative-driven videos by leveraging both visual and textual information.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F16" title="Figure 16 ‣ 3.5.2 Image Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">16</span></a>, we show AI-generated videos of “a Shiba Inu wearing a beret and turtleneck”, “a unique monster family”, “a cloud forming the word ‘SORA”’ and “surfers navigating a tidal wave inside a historic hall”. These examples demonstrate what can be achieved by prompting <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS2.p1.1.2">Sora</span> with DALL·E-generated images.
</p>
</div>
<figure class="ltx_figure" id="S3.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="193" id="S3.F16.g1" src="extracted/5432776/figures/image_prompt.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>This example illustrates the image prompts to guide <span class="ltx_text ltx_font_typewriter" id="S3.F16.2.1">Sora</span>’s text-to-video model to generation. The red boxes visually anchor the key elements of each scene—monsters of varied designs, a cloud formation spelling “SORA”, and surfers in an ornate hall facing a massive tidal wave.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3 </span>Video Prompt</h4>
<div class="ltx_para" id="S3.SS5.SSS3.p1">
<p class="ltx_p" id="S3.SS5.SSS3.p1.1">Video prompts can also be used for video generation
as demonstrated in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib83" title="">83</a>]</cite>.
Recent works (e.g., Moonshot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib84" title="">84</a>]</cite> and Fast-Vid2Vid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib85" title="">85</a>]</cite>) show that good video prompts need to be specific and flexible.
This ensures that the model receives clear direction on specific objectives, like the portrayal of particular objects and visual themes, and also allows for imaginative variations in the final output. For example, in the video extension tasks, a prompt could specify the direction (forward or backward in time) and the context or theme of the extension.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F17" title="Figure 17 ‣ 3.5.3 Video Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">17</span></a>(a), the video prompt instructs <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS3.p1.1.1">Sora</span> to extend a video backward in time to explore the events leading up to the original starting point.
When performing video-to-video editing through video prompts, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F17" title="Figure 17 ‣ 3.5.3 Video Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">17</span></a>(b), the model needs to clearly understand the desired transformation, such as changing the video’s style, setting or atmosphere, or altering subtle aspects like lighting or mood.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S3.F17" title="Figure 17 ‣ 3.5.3 Video Prompt ‣ 3.5 Prompt Engineering ‣ 3 Technology ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">17</span></a>(c), the prompt instructs <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS3.p1.1.2">Sora</span> to connect videos while ensuring smooth transitions between objects in different scenes across videos.</p>
</div>
<figure class="ltx_figure" id="S3.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S3.F17.g1" src="extracted/5432776/figures/video_prompt.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>These examples illustrate the video prompt techniques for <span class="ltx_text ltx_font_typewriter" id="S3.F17.2.1">Sora</span> models: (a) Video Extension, where the model extrapolates the sequence backward the original footage, (b) Video Editing, where specific elements like the setting are transformed as per the text prompt, and (c) Video Connection, where two distinct video prompts are seamlessly blended to create a coherent narrative. Each process is guided by a visual anchor, marked by a red box, ensuring continuity and precision in the generated video content.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.4 </span>Discussion</h4>
<div class="ltx_para" id="S3.SS5.SSS4.p1">
<p class="ltx_p" id="S3.SS5.SSS4.p1.1">Prompt engineering allows users to guide AI models to generate content that aligns with their intent. As an example, the combined use of text, image, and video prompts enables <span class="ltx_text ltx_font_typewriter" id="S3.SS5.SSS4.p1.1.1">Sora</span> to create content that is not only visually compelling but also aligned well with users’ expectations and intent.
While previous studies on prompt engineering have been focused on text and image prompts for LLMs and LVMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib88" title="">88</a>]</cite>,
we expect that there will be a growing interest in video prompts for video generation models.
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Trustworthiness</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">With the rapid advancement of sophisticated models such as ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib89" title="">89</a>]</cite>, GPT4-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib90" title="">90</a>]</cite>, and <span class="ltx_text ltx_font_typewriter" id="S3.SS6.p1.1.1">Sora</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>, the capabilities of these models have seen remarkable enhancements. These developments have made significant contributions to improving work efficiency and propelling technological progress. However, these advancements also raise concerns about the potential for misuse of these technologies, including the generation of fake news <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib92" title="">92</a>]</cite>, privacy breaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib93" title="">93</a>]</cite>, and ethical dilemmas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib95" title="">95</a>]</cite>. Consequently, the issue of trustworthiness in large models has garnered extensive attention from both the academic and industrial spheres, emerging as a focal point of contemporary research discussions.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.1 </span>Safety Concern</h4>
<div class="ltx_para" id="S3.SS6.SSS1.p1">
<p class="ltx_p" id="S3.SS6.SSS1.p1.1">One primary area of focus is the model’s safety, specifically its resilience against misuse and so-called “jailbreak” attacks, where users attempt to exploit vulnerabilities to generate prohibited or harmful content <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib105" title="">105</a>]</cite>. For instance, AutoDAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib103" title="">103</a>]</cite>, a novel and interpretable adversarial attack method based on gradient techniques, is introduced to enable system bypass. In a recent study, researchers explore two reasons why LLMs struggle to resist jailbreak attacks: competing objectives and mismatched generalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib106" title="">106</a>]</cite>. Besides textual attacks, visual jailbreak also threatens the safety of multimodal models (e.g., GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib90" title="">90</a>]</cite>, and <span class="ltx_text ltx_font_typewriter" id="S3.SS6.SSS1.p1.1.1">Sora</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>). A recent study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib107" title="">107</a>]</cite> found that large multimodal models are more vulnerable since the continuous and high-dimensional nature of the additional visual input makes it weaker against adversarial attacks, representing an expanded attack surface.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.2 </span>Other Exploitation</h4>
<div class="ltx_para" id="S3.SS6.SSS2.p1">
<p class="ltx_p" id="S3.SS6.SSS2.p1.1">Due to the large scale of the training dataset and training methodology of large foundation models (e.g., ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib89" title="">89</a>]</cite> and <span class="ltx_text ltx_font_typewriter" id="S3.SS6.SSS2.p1.1.1">Sora</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>), the truthfulness of these models needs to be enhanced as the related issues like hallucination have been discussed widely <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib108" title="">108</a>]</cite>. Hallucination in this context refers to the models’ tendency to generate responses that may appear convincing but are unfounded or false <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib96" title="">96</a>]</cite>. This phenomenon raises critical questions about the reliability and trustworthiness of model outputs, necessitating a comprehensive approach to both evaluate and address the issue. Amount of studies have been dedicated to dissecting the problem of hallucination from various angles. This includes efforts aimed at evaluating the extent and nature of hallucination across different models and scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib110" title="">110</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib111" title="">111</a>]</cite>. These evaluations provide invaluable insights into how and why hallucinations occur, laying the groundwork for developing strategies to mitigate their incidence. Concurrently, a significant body of research is focused on devising and implementing methods to reduce hallucinations in these large models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib112" title="">112</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib114" title="">114</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS2.p2">
<p class="ltx_p" id="S3.SS6.SSS2.p2.1">Another vital aspect of trustworthiness is fairness and bias. The critical importance of developing models that do not perpetuate or exacerbate societal biases is a paramount concern. This priority stems from the recognition that biases encoded within these models can reinforce existing social inequities, leading to discriminatory outcomes. Studies in this area, as evidenced by the work of Gallegos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib115" title="">115</a>]</cite>, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib116" title="">116</a>]</cite>, Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib117" title="">117</a>]</cite>, and Friedrich et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib118" title="">118</a>]</cite>, are dedicated to the meticulous identification and rectification of these inherent biases. The goal is to cultivate models that operate fairly, treating all individuals equitably without bias towards race, gender, or other sensitive attributes. This involves not only detecting and mitigating bias in datasets but also designing algorithms that can actively counteract the propagation of such biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib119" title="">119</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib120" title="">120</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS2.p3">
<p class="ltx_p" id="S3.SS6.SSS2.p3.1">Privacy preservation emerges as another foundational pillar when these models are deployed. In an era where data privacy concerns are escalating, the emphasis on protecting user data has never been more critical. The increasing public awareness and concern over how personal data is handled have prompted more rigorous evaluations of large models. These evaluations focus on the models’ capacity to protect user data, ensuring that personal information remains confidential and is not inadvertently disclosed. Research by Mireshghallah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib121" title="">121</a>]</cite>, Plant et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib122" title="">122</a>]</cite>, and Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib123" title="">123</a>]</cite> exemplify efforts to advance methodologies and technologies that safeguard privacy.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.3 </span>Alignment</h4>
<div class="ltx_para" id="S3.SS6.SSS3.p1">
<p class="ltx_p" id="S3.SS6.SSS3.p1.1">In addressing these challenges, ensuring the trustworthiness of large models has become one of the primary concerns for researchers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib124" title="">124</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib125" title="">125</a>]</cite>. Among the most important technologies is model alignment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib125" title="">125</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib126" title="">126</a>]</cite>, which refers to the process and goal of ensuring that the behavior and outputs of models are consistent with the intentions and ethical standards of human designers. This concerns the development of technology, its moral responsibilities, and social values. In the domain of LLMs, the method of Reinforcement Learning with Human Feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib127" title="">127</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib128" title="">128</a>]</cite> has been widely applied for model alignment. This method combines Reinforcement Learning (RL) with direct human feedback, allowing models to better align with human expectations and standards in understanding and performing tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.4 </span>Discussion</h4>
<div class="ltx_para" id="S3.SS6.SSS4.p1">
<p class="ltx_p" id="S3.SS6.SSS4.p1.1">From <span class="ltx_text ltx_font_typewriter" id="S3.SS6.SSS4.p1.1.1">Sora</span> (specifically its technical report), we summarize some insightful findings that potentially offer an informative guideline for future work:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS4.p2">
<p class="ltx_p" id="S3.SS6.SSS4.p2.1">(1) <span class="ltx_text ltx_font_italic" id="S3.SS6.SSS4.p2.1.1">Integrated Protection of Model and External Security</span>: As models become more powerful, especially in generating content, ensuring that they are not misused to produce harmful content (such as hate speech <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib129" title="">129</a>]</cite> and false information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib91" title="">91</a>]</cite>) has become a serious challenge. In addition to aligning the model itself, external security protections are equally important. This includes content filtering and review mechanisms, usage permissions and access control, data privacy protection, as well as enhancements in transparency and explainability. For instance, OpenAI now uses a detection classifier to tell whether a given video is generated by <span class="ltx_text ltx_font_typewriter" id="S3.SS6.SSS4.p2.1.2">Sora</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib130" title="">130</a>]</cite>. Moreover, a text classifier is deployed to detect the potentially harmful textual input <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib130" title="">130</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS4.p3">
<p class="ltx_p" id="S3.SS6.SSS4.p3.1">(2) <span class="ltx_text ltx_font_italic" id="S3.SS6.SSS4.p3.1.1">Security Challenges of Multimodal Models</span>: Multimodal models, such as text-to-video models like <span class="ltx_text ltx_font_typewriter" id="S3.SS6.SSS4.p3.1.2">Sora</span> bring additional complexity to security due to their ability to understand and generate various types of content (text, images, videos, etc.). Multimodal models can produce content in various forms, increasing the ways and scope of misuse and copyright issues. As the content generated by multimodal models is more complex and diverse, traditional methods of content verification and authenticity may no longer be effective. This requires the development of new technologies and methods to identify and filter harmful content generated by these models, increasing the difficulty of regulation and management.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS4.p4">
<p class="ltx_p" id="S3.SS6.SSS4.p4.1">(3) <span class="ltx_text ltx_font_italic" id="S3.SS6.SSS4.p4.1.1">The Need for Interdisciplinary Collaboration</span>: Ensuring the safety of models is not just a technical issue but also requires cross-disciplinary cooperation. To address these challenges, experts from various fields such as law <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib131" title="">131</a>]</cite> and psychology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib132" title="">132</a>]</cite> need to work together to develop appropriate norms (e.g., what’s the safety and what’s unsafe?), policies, and technological solutions. The need for interdisciplinary collaboration significantly increases the complexity of solving these issues.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Applications</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As video diffusion models, exemplified by <span class="ltx_text ltx_font_typewriter" id="S4.p1.1.1">Sora</span>, emerge as a forefront technology, their adoption across diverse research fields and industries is rapidly accelerating. The implications of this technology extend far beyond mere video creation, offering transformative potential for tasks ranging from automated content generation to complex decision-making processes. In this section, we delve into a comprehensive examination of the current applications of video diffusion models, highlighting key areas where <span class="ltx_text ltx_font_typewriter" id="S4.p1.1.2">Sora</span> has not only demonstrated its capabilities but also revolutionized the approach to solving complex problems. We aim to offer a broad perspective for the practical deployment scenarios (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#S4.F18" title="Figure 18 ‣ 4 Applications ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">18</span></a>).</p>
</div>
<figure class="ltx_figure" id="S4.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="234" id="S4.F18.g1" src="extracted/5432776/figures/Sora_application.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Applications of <span class="ltx_text ltx_font_typewriter" id="S4.F18.2.1">Sora</span>.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Movie</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Traditionally, creating cinematic masterpieces has been an arduous and expensive process, often requiring decades of effort, cutting-edge equipment, and substantial financial investments. However, the advent of advanced video generation technologies heralds a new era in film-making, one where the dream of autonomously producing movies from simple text inputs is becoming a reality. Researchers have ventured into the realm of movie generation by extending video generation models into creating movies. MovieFactory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib133" title="">133</a>]</cite> applies diffusion models to generate film-style videos from elaborate scripts produced by ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib89" title="">89</a>]</cite>, representing a significant leap forward. In the follow-up, MobileVidFactory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib134" title="">134</a>]</cite> can automatically generate vertical mobile videos with only simple texts provided by users. Vlogger <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib135" title="">135</a>]</cite> makes it feasible for users to compose a minute-long vlog. These developments, epitomized by <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.1">Sora</span>’s ability to generate captivating movie content effortlessly, mark a pivotal moment in the democratization of movie production. They offer a glimpse into a future where anyone can be a filmmaker, significantly lowering the barriers to entry in the film industry and introducing a novel dimension to movie production that blends traditional storytelling with AI-driven creativity. The implications of these technologies extend beyond simplification. They promise to reshape the landscape of film production, making it more accessible and versatile in the face of evolving viewer preferences and distribution channels.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Education</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The landscape of educational content has long been dominated by static resources, which, despite their value, often fall short of catering to the diverse needs and learning styles of today’s students. Video diffusion models stand at the forefront of an educational revolution, offering unprecedented opportunities to customize and animate educational materials in ways that significantly enhance learner engagement and understanding. These advanced technologies enable educators to transform text descriptions or curriculum outlines into dynamic, engaging video content tailored to the specific style, and interests of individual learners <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib136" title="">136</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib137" title="">137</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib138" title="">138</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib139" title="">139</a>]</cite>. Moreover, image-to-video editing techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib140" title="">140</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib141" title="">141</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib142" title="">142</a>]</cite> present innovative avenues for converting static educational assets into interactive videos, thereby supporting a range of learning preferences and potentially increasing student engagement. By integrating these models into educational content creation, educators can produce videos on a myriad of subjects, making complex concepts more accessible and captivating for students. The use of <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.1">Sora</span> in revolutionizing the educational domain exemplifies the transformative potential of these technologies. This shift towards personalized, dynamic educational content heralds a new era in education.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Gaming</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The gaming industry constantly seeks ways to push the boundaries of realism and immersion, yet traditional game development often grapples with the limitations of pre-rendered environments and scripted events. The generation of dynamic, high-fidelity video content and realistic sound by diffusion models effects in real-time, promise to overcome existing constraints, offering developers the tools to create evolving game environments that respond organically to player actions and game events <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib143" title="">143</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib144" title="">144</a>]</cite>. This could include generating changing weather conditions, transforming landscapes, or even creating entirely new settings on the fly, making game worlds more immersive and responsive.
Some methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib145" title="">145</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib146" title="">146</a>]</cite> also synthesize realistic impact sounds from video inputs, enhancing game audio experiences. With the integration of <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.1">Sora</span> within the gaming domain, unparalleled immersive experiences that captivate and engage players can be created. How games are developed, played, and experienced will be innovated, as well as opening new possibilities for storytelling, interaction, and immersion.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Healthcare</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Despite generative capabilities, video diffusion models excel in understanding and generating complex video sequences, making them particularly suited for identifying dynamic anomalies within the body, such as early cellular apoptosis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib147" title="">147</a>]</cite>, skin lesion progression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib148" title="">148</a>]</cite>, and irregular human movements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib149" title="">149</a>]</cite>, which are crucial for early disease detection and intervention strategies. Additionally, models like MedSegDiff-V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib150" title="">150</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib151" title="">151</a>]</cite> leverage the power of transformers to segment medical images with unprecedented precision, enabling clinicians to pinpoint areas of interest across various imaging modalities with enhanced accuracy.
The integration of <span class="ltx_text ltx_font_typewriter" id="S4.SS4.p1.1.1">Sora</span> into clinical practice promises not only to refine diagnostic processes but also to personalize patient care, offering tailored treatment plans based on precise medical imaging analysis. However, this technological integration comes with its own set of challenges, including the need for robust data privacy measures and addressing ethical considerations in healthcare.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Robotics</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Video diffusion models now play important roles in robotics, showing a new era where robots can generate and interpret complex video sequences for enhanced perception <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib152" title="">152</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib153" title="">153</a>]</cite> and decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib155" title="">155</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib156" title="">156</a>]</cite>. These models unlock new capabilities in robots, enabling them to interact with their environment and execute tasks with unprecedented complexity and precision. The introduction of web-scale diffusion models to robotics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib152" title="">152</a>]</cite> showcases the potential for leveraging large-scale models to enhance robotic vision and understanding. Latent diffusion models are employed for language-instructed video prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib157" title="">157</a>]</cite>, allowing robots to understand and execute tasks by predicting the outcome of actions in video format.
Furthermore, the reliance on simulated environments for robotics research has been innovatively addressed by video diffusion models capable of creating highly realistic video sequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib158" title="">158</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib159" title="">159</a>]</cite>. This enables the generation of diverse training scenarios for robots, mitigating the limitations imposed by the scarcity of real-world data.
We believe, the integration of technologies like <span class="ltx_text ltx_font_typewriter" id="S4.SS5.p1.1.1">Sora</span> into the robotics field holds the promise of groundbreaking developments. By harnessing the power of <span class="ltx_text ltx_font_typewriter" id="S4.SS5.p1.1.2">Sora</span>, the future of robotics is poised for unprecedented advancements, where robots can seamlessly navigate and interact with their environments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_typewriter" id="S5.p1.1.1">Sora</span> shows a remarkable talent for precisely understanding and implementing complex instructions from humans. This model excels at creating detailed videos with various characters, all set within elaborately crafted settings. A particularly impressive attribute of <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.2">Sora</span> is its ability to produce videos up to one minute in length while ensuring consistent and engaging storytelling. This marks a significant improvement over previous attempts that focused on shorter video pieces, as <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.3">Sora</span>’s extended sequences exhibit a clear narrative flow and maintain visual consistency from start to finish. Furthermore, <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.4">Sora</span> distinguishes itself by generating longer video sequences that capture complex movements and interactions, advancing past the restrictions of earlier models that could only handle short clips and basic images. This advancement signifies a major step forward in AI-powered creative tools, enabling users to transform written stories into vivid videos with a level of detail and sophistication that was previously unattainable.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Limitations</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Challenges in Physical Realism.</span> <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.2">Sora</span>, as a simulation platform, exhibits a range of limitations that undermine its effectiveness in accurately depicting complex scenarios. Most important is its inconsistent handling of physical principles within complex scenes, leading to a failure in accurately copying specific examples of cause and effect. For instance, consuming a portion of a cookie might not result in a corresponding bite mark, illustrating the system’s occasional departure from physical plausibility. This issue extends to the simulation of motion, where <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.3">Sora</span> generates movements that challenge realistic physical modeling, such as unnatural transformations of objects or the incorrect simulation of rigid structures like chairs, leading to unrealistic physical interactions. The challenge further increases when simulating complex interactions among objects and characters, occasionally producing outcomes that lean towards the humorous.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Spatial and Temporal Complexities.</span> <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.2">Sora</span> occasionally misunderstands instructions related to the placement or arrangement of objects and characters within a given prompt, leading to confusion about directions (e.g., confusing left for right). Additionally, it faces challenges in maintaining the temporal accuracy of events, particularly when it comes to adhering to designated camera movements or sequences. This can result in deviating from the intended temporal flow of scenes. In complex scenarios that involve a multitude of characters or elements, <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.3">Sora</span> has a tendency to insert irrelevant animals or people. Such additions can significantly change the originally envisioned composition and atmosphere of the scene, moving away from the planned narrative or visual layout. This issue not only affects the model’s ability to accurately recreate specific scenes or narratives but also impacts its reliability in generating content that closely aligns with the user’s expectations and the coherence of the generated output.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Limitations in Human-computer Interaction (HCI).</span> <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.2">Sora</span>, while showing potential in the video generation domain, faces significant limitations in HCI. These limitations are primarily evident in the coherence and efficiency of user-system interactions, especially when making detailed modifications or optimizations to generated content. For instance, users might find it difficult to precisely specify or adjust the presentation of specific elements within a video, such as action details and scene transitions. Additionally, <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.3">Sora</span>’s limitations in understanding complex language instructions or capturing subtle semantic differences could result in video content that does not fully meet user expectations or needs. These shortcomings restrict <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.4">Sora</span>’s potential in video editing and enhancement, also impacting the overall satisfaction of the user experience.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Usage Limitation.</span>
Regarding usage limitations, OpenAI has not yet set a specific release date for public access to <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p4.1.2">Sora</span>, emphasizing a cautious approach towards safety and readiness before broad deployment. This indicates that further improvements and testing in areas such as security, privacy protection, and content review may still be necessary for <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p4.1.3">Sora</span>. Moreover, at present, <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p4.1.4">Sora</span> can only generate videos up to one minute in length, and according to published cases, most generated videos are only a few dozen seconds long. This limitation restricts its use in applications requiring longer content display, such as detailed instructional videos or in-depth storytelling. This limitation reduces <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p4.1.5">Sora</span>’s flexibility in the content creation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Opportunities</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">Academy.</span>
(1) The introduction of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.1.2">Sora</span> by OpenAI marks a strategic shift towards encouraging the broader AI community to delve deeper into the exploration of text-to-video models, leveraging both diffusion and transformer technologies. This initiative aims to redirect the focus toward the potential of creating highly sophisticated and nuanced video content directly from textual descriptions, a frontier that promises to revolutionize content creation, storytelling, and information sharing. (2) The innovative approach of training <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.1.3">Sora</span> on data at its native size, as opposed to the traditional methods of resizing or cropping, serves as a groundbreaking inspiration for the academic community. It opens up new pathways by highlighting the benefits of utilizing unmodified datasets, which leads to the creation of more advanced generative models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Industry.</span>
(1) The current capabilities of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.2">Sora</span> signal a promising path for the advancement of video simulation technologies, highlighting the potential to significantly enhance realism within both physical and digital areas. The prospect of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.3">Sora</span> enabling the creation of highly realistic environments through textual descriptions presents a promising future for content creation. This potential extends to revolutionizing game development, offering a glimpse into a future where immersive-generated worlds can be crafted with unprecedented ease and accuracy. (2) Companies may leverage <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.4">Sora</span> to produce advertising videos that swiftly adapt to market changes and create customized marketing content. This not only reduces production costs but also enhances the appeal and effectiveness of advertisements. The ability of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.5">Sora</span> to generate highly realistic video content from textual descriptions alone could revolutionize how brands engage with their audience, allowing for the creation of immersive and compelling videos that capture the essence of their products or services in unprecedented ways.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Society.</span>
(1) While the prospect of utilizing text-to-video technology to replace traditional filmmaking remains distant, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.2">Sora</span> and similar platforms hold transformative potential for content creation on social media. The constraints of current video lengths do not diminish the impact these tools can have in making high-quality video production accessible to everyone, enabling individuals to produce compelling content without the need for expensive equipment. It represents a significant shift towards empowering content creators across platforms like TikTok and Reels, bringing in a new age of creativity and engagement. (2) Screenwriters and creative professionals can use <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.3">Sora</span> to transform written scripts into videos, assisting them in better showing and sharing their creative concepts, and even in producing short films and animations. The ability to create detailed, vivid videos from scripts can fundamentally change the pre-production process of filmmaking and animation, offering a glimpse into how future storytellers might pitch, develop, and refine their narratives. This technology opens up possibilities for a more dynamic and interactive form of script development, where ideas can be visualized and assessed in real time, providing a powerful tool for creativity and collaboration. (3) Journalists and news organizations can also utilize <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.4">Sora</span> to quickly generate news reports or explanatory videos, making the news content more vivid and engaging. This can significantly increase the coverage and audience engagement of news reports. By providing a tool that can simulate realistic environments and scenarios, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.5">Sora</span> offers a powerful solution for visual storytelling, enabling journalists to convey complex stories through engaging videos that were previously difficult or expensive to produce. In summary, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.6">Sora</span>’s potential to revolutionize content creation across marketing, journalism, and entertainment is immense.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We present a comprehensive review of <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.1">Sora</span> to help developers and researchers study the capabilities and related works of <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.2">Sora</span>.
The review is based on our survey of published technical reports and reverse engineering based on existing literature.
We will continue to update the paper when <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.3">Sora</span>’s API is available and further details about <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.4">Sora</span> are revealed.
We hope that this review paper will prove a valuable resource for the open-source research community and lay a foundation for the community to jointly develop an open-source version of <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.5">Sora</span> in the near future to democratize video auto-creation in the era of AIGC. To achieve this goal, we invite discussions, suggestions, and collaborations on all fronts.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
OpenAI, “Chatgpt: Get instant answers, find creative inspiration, learn something new..” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/chatgpt" title="">https://openai.com/chatgpt</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
OpenAI, “Gpt-4 technical report,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
OpenAI, “Sora: Creating video from text.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/sora" title="">https://openai.com/sora</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W. Peebles and S. Xie, “Scalable diffusion models with transformers,” in <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 4195–4205, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. A. Efros and T. K. Leung, “Texture synthesis by non-parametric sampling,” in <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the seventh IEEE international conference on computer vision</span>, vol. 2, pp. 1033–1038, IEEE, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
P. S. Heckbert, “Survey of texture mapping,” <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE computer graphics and applications</span>, vol. 6, no. 11, pp. 56–67, 1986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:1312.6114</span>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
L. Dinh, D. Krueger, and Y. Bengio, “Nice: Non-linear independent components estimation,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1410.8516</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Song and S. Ermon, “Generative modeling by estimating gradients of the data distribution,” <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</span>, vol. 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, “A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2303.04226</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Advances in Neural Information Processing Systems</span> (I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30, Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">et al.</span>, “Improving language understanding by generative pre-training,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">et al.</span>, “An image is worth 16x16 words: Transformers for image recognition at scale,” <span class="ltx_text ltx_font_italic" id="bib.bib15.2.2">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF international conference on computer vision</span>, pp. 10012–10022, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</span>, pp. 234–241, Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. AI, “Midjourney: Text to image with ai art generator.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.midjourneyai.ai/en" title="">https://www.midjourneyai.ai/en</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">et al.</span>, “Improving image generation with better captions,” <span class="ltx_text ltx_font_italic" id="bib.bib21.2.2">Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf</span>, vol. 2, p. 3, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
P. AI, “Pika is the idea-to-video platform that sets your creativity in motion..” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pika.art/home" title="">https://pika.art/home</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. AI, “Gen-2: Gen-2: The next step forward for generative ai.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://research.runwayml.com/gen2" title="">https://research.runwayml.com/gen2</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision transformers,” in <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 12104–12113, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">et al.</span>, “Scaling vision transformers to 22 billion parameters,” in <span class="ltx_text ltx_font_italic" id="bib.bib25.2.2">International Conference on Machine Learning</span>, pp. 7480–7512, PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">et al.</span>, “Learning transferable visual models from natural language supervision,” in <span class="ltx_text ltx_font_italic" id="bib.bib26.2.2">International conference on machine learning</span>, pp. 8748–8763, PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">et al.</span>, “Stable video diffusion: Scaling latent video diffusion models to large datasets,” <span class="ltx_text ltx_font_italic" id="bib.bib27.2.2">arXiv preprint arXiv:2311.15127</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, and Y. Taigman, “Make-a-video: Text-to-video generation without text-video data,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">et al.</span>, “Imagen video: High definition video generation with diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib29.2.2">arXiv preprint arXiv:2210.02303</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
R. Sutton, “The bitter lesson.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" title="">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>, March 2019.

</span>
<span class="ltx_bibblock">Accessed: Your Access Date Here.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Xie, “Take on sora technical report.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://twitter.com/sainingxie/status/1758433676105310543" title="">https://twitter.com/sainingxie/status/1758433676105310543</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Van Den Oord, O. Vinyals, <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">et al.</span>, “Neural discrete representation learning,” <span class="ltx_text ltx_font_italic" id="bib.bib32.2.2">Advances in neural information processing systems</span>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 16000–16009, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Ge, S. Nah, G. Liu, T. Poon, A. Tao, B. Catanzaro, D. Jacobs, J.-B. Huang, M.-Y. Liu, and Y. Balaji, “Preserve your own correlation: A noise prior for video diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 22930–22941, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach, “Adversarial diffusion distillation,” <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2311.17042</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, “Align your latents: High-resolution video synthesis with latent diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 22563–22575, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Ryoo, A. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova, “Tokenlearner: Adaptive space-time tokenization for videos,” <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Advances in Neural Information Processing Systems</span>, vol. 34, pp. 12786–12797, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid, “Vivit: A video vision transformer,” <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2103.15691</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
L. Beyer, P. Izmailov, A. Kolesnikov, M. Caron, S. Kornblith, X. Zhai, M. Minderer, M. Tschannen, I. Alabdulmohsin, and F. Pavetic, “Flexivit: One model for all patch sizes,” in <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 14496–14506, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M. Dehghani, B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. Steiner, J. Puigcerver, R. Geirhos, I. M. Alabdulmohsin, <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">et al.</span>, “Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution,” <span class="ltx_text ltx_font_italic" id="bib.bib40.2.2">Advances in Neural Information Processing Systems</span>, vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. M. Krell, M. Kosec, S. P. Perez, and A. Fitzgibbon, “Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance,” <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2107.02027</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
H. Yin, A. Vahdat, J. M. Alvarez, A. Mallya, J. Kautz, and P. Molchanov, “A-vit: Adaptive tokens for efficient vision transformer,” in <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 10809–10818, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman, “Token merging: Your vit but faster,” in <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">The Eleventh International Conference on Learning Representations</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
M. Fayyaz, S. A. Koohpayegani, F. R. Jafari, S. Sengupta, H. R. V. Joze, E. Sommerlade, H. Pirsiavash, and J. Gall, “Adaptive token sampling for efficient vision transformers,” in <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">European Conference on Computer Vision</span>, pp. 396–414, Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Advances in neural information processing systems</span>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention all you need for video understanding?,” in <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">ICML</span>, vol. 2, p. 4, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
L. Yu, J. Lezama, N. B. Gundavarapu, L. Versari, K. Sohn, D. Minnen, Y. Cheng, A. Gupta, X. Gu, A. G. Hauptmann, <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">et al.</span>, “Language model beats diffusion–tokenizer is key to visual generation,” <span class="ltx_text ltx_font_italic" id="bib.bib47.2.2">arXiv preprint arXiv:2310.05737</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
N. Shazeer, “Fast transformer decoding: One write-head is all you need,” 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai, “Gqa: Training generalized multi-query transformer models from multi-head checkpoints,” <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2305.13245</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2312.00752</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep unsupervised learning using nonequilibrium thermodynamics,” <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:1503.03585</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Advances in Neural Information Processing Systems</span>, vol. 33, pp. 6840–6851, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, “Score-based generative modeling through stochastic differential equations,” <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2011.13456</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu, “All are worth words: A vit backbone for diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
S. Gao, P. Zhou, M.-M. Cheng, and S. Yan, “Masked diffusion transformer is a strong image synthesizer,” <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2303.14389</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
A. Hatamizadeh, J. Song, G. Liu, J. Kautz, and A. Vahdat, “Diffit: Diffusion vision transformers for image generation,” <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2312.02139</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
J. Ho and T. Salimans, “Classifier-free diffusion guidance,” <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2207.12598</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
T. Salimans and J. Ho, “Progressive distillation for fast sampling of diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2202.00512</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans, “Cascaded diffusion models for high fidelity image generation,” <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">The Journal of Machine Learning Research</span>, vol. 23, no. 1, pp. 2249–2281, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach, “Sdxl: Improving latent diffusion models for high-resolution image synthesis,” <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2307.01952</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">et al.</span>, “Language models are few-shot learners,” <span class="ltx_text ltx_font_italic" id="bib.bib62.2.2">arXiv</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Conditional prompt learning for vision-language models,” in <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 16816–16825, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">et al.</span>, “Multitask prompted training enables zero-shot task generalization,” <span class="ltx_text ltx_font_italic" id="bib.bib64.2.2">arXiv preprint arXiv:2110.08207</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot learners,” <span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2109.01652</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">et al.</span>, “Training language models to follow instructions with human feedback,” <span class="ltx_text ltx_font_italic" id="bib.bib66.2.2">Advances in Neural Information Processing Systems</span>, vol. 35, pp. 27730–27744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language representation learning with noisy text supervision,” in <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">International conference on machine learning</span>, pp. 4904–4916, PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu, “Coca: Contrastive captioners are image-text foundation models,” <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2205.01917</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu, and J. Yu, “Video-text modeling with zero-shot transfer from contrastive captioners,” <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2212.04979</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">et al.</span>, “mplug-2: A modularized multi-modal foundation model across text, image and video,” <span class="ltx_text ltx_font_italic" id="bib.bib70.2.2">arXiv preprint arXiv:2302.00402</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang, “Git: A generative image-to-text transformer for vision and language,” <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2205.14100</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, “Zero-shot video question answering via frozen bidirectional language models,” <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">Advances in Neural Information Processing Systems</span>, vol. 35, pp. 124–141, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Y. Li, “A practical survey on zero-shot prompt design for in-context learning,” in <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Proceedings of the Conference Recent Advances in Natural Language Processing - Large Language Models for Natural Language Processings</span>, RANLP, INCOMA Ltd., Shoumen, BULGARIA, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
B. Chen, Z. Zhang, N. Langrené, and S. Zhu, “Unleashing the potential of prompt engineering in large language models: a comprehensive review,” <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2310.14735</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
S. Pitis, M. R. Zhang, A. Wang, and J. Ba, “Boosted prompt ensembles for large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Y. Hao, Z. Chi, L. Dong, and F. Wei, “Optimizing prompts for text-to-image generation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
S. Huang, B. Gong, Y. Pan, J. Jiang, Y. Lv, Y. Li, and D. Wang, “Vop: Text-video co-operative prompt tuning for cross-modal retrieval,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
J. Z. Wu, Y. Ge, X. Wang, W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou, “Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
T. Lüddecke and A. Ecker, “Image segmentation using text and image prompts,” in <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 7086–7096, June 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
X. Chen, Y. Wang, L. Zhang, S. Zhuang, X. Ma, J. Yu, Y. Wang, D. Lin, Y. Qiao, and Z. Liu, “Seine: Short-to-long video diffusion model for generative transition and prediction,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
H. Chen, Y. Zhang, X. Cun, M. Xia, X. Wang, C. Weng, and Y. Shan, “Videocrafter2: Overcoming data limitations for high-quality video diffusion models,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz, and B. Catanzaro, “Video-to-video synthesis,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
T.-C. Wang, M.-Y. Liu, A. Tao, G. Liu, J. Kautz, and B. Catanzaro, “Few-shot video-to-video synthesis,” 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
D. J. Zhang, D. Li, H. Le, M. Z. Shou, C. Xiong, and D. Sahoo, “Moonshot: Towards controllable video generation and editing with multimodal conditions,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
L. Zhuo, G. Wang, S. Li, W. Wu, and Z. Liu, “Fast-vid2vid: Spatial-temporal compression for video-to-video synthesis,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-efficient prompt tuning,” in <span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pp. 3045–3059, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, “Visual prompt tuning,” in <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">European Conference on Computer Vision</span>, pp. 709–727, Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
OpenAI, “Introducing chatgpt,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
OpenAI, “Gpt-4v(ision) system card,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Y. Huang and L. Sun, “Harnessing the power of chatgpt in fake news: An in-depth exploration in generation, detection and explanation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
C. Chen and K. Shu, “Can llm-generated misinformation be detected?,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Huang, X. Yu, L. Zhang, Z. Wu, C. Cao, H. Dai, L. Zhao, Y. Li, P. Shu, F. Zeng, L. Sun, W. Liu, D. Shen, Q. Li, T. Liu, D. Zhu, and X. Li, “Deid-gpt: Zero-shot medical text de-identification by gpt-4,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Yao, X. Yi, X. Wang, Y. Gong, and X. Xie, “Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Y. Huang, Q. Zhang, P. S. Y, and L. Sun, “Trustgpt: A benchmark for trustworthy and responsible large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu, Y. Zhang, X. Li, Z. Liu, Y. Liu, Y. Wang, Z. Zhang, B. Kailkhura, C. Xiong, C. Xiao, C. Li, E. Xing, F. Huang, H. Liu, H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis, M. Zitnik, M. Jiang, M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao, J. Tang, J. Wang, J. Mitchell, K. Shu, K. Xu, K.-W. Chang, L. He, L. Huang, M. Backes, N. Z. Gong, P. S. Yu, P.-Y. Chen, Q. Gu, R. Xu, R. Ying, S. Ji, S. Jana, T. Chen, T. Liu, T. Zhou, W. Wang, X. Li, X. Zhang, X. Wang, X. Xie, X. Chen, X. Wang, Y. Liu, Y. Ye, Y. Cao, Y. Chen, and Y. Zhao, “Trustllm: Trustworthiness in large language models,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li, D. Forsyth, and D. Hendrycks, “Harmbench: A standardized evaluation framework for automated red teaming and robust refusal,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Y. Wang, H. Li, X. Han, P. Nakov, and T. Baldwin, “Do-not-answer: A dataset for evaluating safeguards in llms,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer, <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">et al.</span>, “Decodingtrust: A comprehensive assessment of trustworthiness in gpt models,” <span class="ltx_text ltx_font_italic" id="bib.bib99.2.2">arXiv preprint arXiv:2306.11698</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Z. Zhang, L. Lei, L. Wu, R. Sun, Y. Huang, C. Long, X. Liu, X. Lei, J. Tang, and M. Huang, “Safetybench: Evaluating the safety of large language models with multiple choice questions,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, “" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models,” <span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2308.03825</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
X. Liu, N. Xu, M. Chen, and C. Xiao, “Autodan: Generating stealthy jailbreak prompts on aligned large language models,” <span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">arXiv preprint arXiv:2310.04451</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
S. Zhu, R. Zhang, B. An, G. Wu, J. Barrow, Z. Wang, F. Huang, A. Nenkova, and T. Sun, “Autodan: Interpretable gradient-based adversarial attacks on large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
A. Zhou, B. Li, and H. Wang, “Robust prompt optimization for defending language models against jailbreaking attacks,” <span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">arXiv preprint arXiv:2401.17263</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
X. Guo, F. Yu, H. Zhang, L. Qin, and B. Hu, “Cold-attack: Jailbreaking llms with stealthiness and controllability,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: How does llm safety training fail?,” <span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:2307.02483</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Z. Niu, H. Ren, X. Gao, G. Hua, and R. Jin, “Jailbreaking attack against multimodal large language model,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
H. Liu, W. Xue, Y. Chen, D. Chen, X. Zhao, K. Wang, L. Hou, R. Li, and W. Peng, “A survey on hallucination in large vision-language models,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob, D. Manocha, and T. Zhou, “Hallusionbench: An advanced diagnostic suite for entangled language hallucination &amp; visual illusion in large vision-language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, “Evaluating object hallucination in large vision-language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Y. Huang, J. Shi, Y. Li, C. Fan, S. Wu, Q. Zhang, Y. Liu, P. Zhou, Y. Wan, N. Z. Gong, <span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">et al.</span>, “Metatool benchmark for large language models: Deciding whether to use tools and which to use,” <span class="ltx_text ltx_font_italic" id="bib.bib111.2.2">arXiv preprint arXiv:2310.03128</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang, “Mitigating hallucination in large multi-modal models via robust instruction tuning,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
L. Wang, J. He, S. Li, N. Liu, and E.-P. Lim, “Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites,” in <span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">International Conference on Multimedia Modeling</span>, pp. 32–45, Springer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao, “Analyzing and mitigating object hallucination in large vision-language models,” <span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">arXiv preprint arXiv:2310.00754</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Dernoncourt, T. Yu, R. Zhang, and N. K. Ahmed, “Bias and fairness in large language models: A survey,” <span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">arXiv preprint arXiv:2309.00770</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
J. Zhang, K. Bao, Y. Zhang, W. Wang, F. Feng, and X. He, “Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation,” <span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">arXiv preprint arXiv:2305.07609</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Y. Liang, L. Cheng, A. Payani, and K. Shu, “Beyond detection: Unveiling fairness vulnerabilities in abusive language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
F. Friedrich, P. Schramowski, M. Brack, L. Struppek, D. Hintersdorf, S. Luccioni, and K. Kersting, “Fair diffusion: Instructing text-to-image generation models on fairness,” <span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:2302.10893</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
R. Liu, C. Jia, J. Wei, G. Xu, L. Wang, and S. Vosoughi, “Mitigating political bias in language models through reinforced calibration,” <span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, vol. 35, pp. 14857–14866, May 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
R. K. Mahabadi, Y. Belinkov, and J. Henderson, “End-to-end bias mitigation by modelling biases in corpora,” 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
N. Mireshghallah, H. Kim, X. Zhou, Y. Tsvetkov, M. Sap, R. Shokri, and Y. Choi, “Can llms keep a secret? testing privacy implications of language models via contextual integrity theory,” <span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">arXiv preprint arXiv:2310.17884</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
R. Plant, V. Giuffrida, and D. Gkatzia, “You are what you write: Preserving privacy in the era of large language models,” <span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">arXiv preprint arXiv:2204.09391</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
H. Li, Y. Chen, J. Luo, Y. Kang, X. Zhang, Q. Hu, C. Chan, and Y. Song, “Privacy in large language models: Attacks, defenses and future directions,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Ré,
D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang, “On the opportunities and risks of foundation models,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong, “Large language model alignment: A survey,” <span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">arXiv preprint arXiv:2309.15025</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam, X. Zhang, L. Sun, H. Wang, J. Zhang, M. Huang, Y. Dong, and J. Tang, “Alignbench: Benchmarking chinese alignment of large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei, “Deep reinforcement learning from human preferences,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
T. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu, H.-T. Zheng, M. Sun, and T.-S. Chua, “Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
M. S. Jahan and M. Oussalah, “A systematic review of hate speech automatic detection using natural language processing.,” <span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">Neurocomputing</span>, p. 126232, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
OpenAI, “Sora safety.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/sora#safety" title="">https://openai.com/sora#safety</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Z. Fei, X. Shen, D. Zhu, F. Zhou, Z. Han, S. Zhang, K. Chen, Z. Shen, and J. Ge, “Lawbench: Benchmarking legal knowledge of large language models,” <span class="ltx_text ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2309.16289</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Y. Li, Y. Huang, Y. Lin, S. Wu, Y. Wan, and L. Sun, “I think, therefore i am: Benchmarking awareness of large language models using awarebench,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
J. Zhu, H. Yang, H. He, W. Wang, Z. Tuo, W.-H. Cheng, L. Gao, J. Song, and J. Fu, “Moviefactory: Automatic movie creation from text using large generative models for language and images,” <span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2306.07257</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
J. Zhu, H. Yang, W. Wang, H. He, Z. Tuo, Y. Yu, W.-H. Cheng, L. Gao, J. Song, J. Fu, <span class="ltx_text ltx_font_italic" id="bib.bib134.1.1">et al.</span>, “Mobilevidfactory: Automatic diffusion-based social media video generation for mobile devices from text,” in <span class="ltx_text ltx_font_italic" id="bib.bib134.2.2">Proceedings of the 31st ACM International Conference on Multimedia</span>, pp. 9371–9373, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
S. Zhuang, K. Li, X. Chen, Y. Wang, Z. Liu, Y. Qiao, and Y. Wang, “Vlogger: Make your dream a vlog,” <span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">arXiv preprint arXiv:2401.09414</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
R. Feng, W. Weng, Y. Wang, Y. Yuan, J. Bao, C. Luo, Z. Chen, and B. Guo, “Ccedit: Creative and controllable video editing via diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">arXiv preprint arXiv:2309.16496</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
J. Xing, M. Xia, Y. Liu, Y. Zhang, Y. Zhang, Y. He, H. Liu, H. Chen, X. Cun, X. Wang, <span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">et al.</span>, “Make-your-video: Customized video generation using textual and structural guidance,” <span class="ltx_text ltx_font_italic" id="bib.bib137.2.2">arXiv preprint arXiv:2306.00943</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Y. Guo, C. Yang, A. Rao, Y. Wang, Y. Qiao, D. Lin, and B. Dai, “Animatediff: Animate your personalized text-to-image diffusion models without specific tuning,” <span class="ltx_text ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:2307.04725</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Y. He, M. Xia, H. Chen, X. Cun, Y. Gong, J. Xing, Y. Zhang, X. Wang, C. Weng, Y. Shan, <span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">et al.</span>, “Animate-a-story: Storytelling with retrieval-augmented video generation,” <span class="ltx_text ltx_font_italic" id="bib.bib139.2.2">arXiv preprint arXiv:2307.06940</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
H. Ni, C. Shi, K. Li, S. X. Huang, and M. R. Min, “Conditional image-to-video generation with latent flow diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 18444–18455, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
L. Hu, X. Gao, P. Zhang, K. Sun, B. Zhang, and L. Bo, “Animate anyone: Consistent and controllable image-to-video synthesis for character animation,” <span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">arXiv preprint arXiv:2311.17117</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Y. Hu, C. Luo, and Z. Chen, “Make it move: controllable image-to-video generation with text descriptions,” in <span class="ltx_text ltx_font_italic" id="bib.bib142.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 18219–18228, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
K. Mei and V. Patel, “Vidm: Video implicit diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, vol. 37, pp. 9117–9125, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
S. Yu, K. Sohn, S. Kim, and J. Shin, “Video probabilistic diffusion models in projected latent space,” in <span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 18456–18466, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
K. Su, K. Qian, E. Shlizerman, A. Torralba, and C. Gan, “Physics-driven diffusion models for impact sound synthesis from videos,” in <span class="ltx_text ltx_font_italic" id="bib.bib145.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 9749–9759, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
S. Li, W. Dong, Y. Zhang, F. Tang, C. Ma, O. Deussen, T.-Y. Lee, and C. Xu, “Dance-to-music generation with encoder-based textual inversion of diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">arXiv preprint arXiv:2401.17800</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
A. Awasthi, J. Nizam, S. Zare, S. Ahmad, M. J. Montalvo, N. Varadarajan, B. Roysam, and H. V. Nguyen, “Video diffusion models for the apoptosis forcasting,” <span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">bioRxiv</span>, pp. 2023–11, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
A. Bozorgpour, Y. Sadegheih, A. Kazerouni, R. Azad, and D. Merhof, “Dermosegdiff: A boundary-aware segmentation diffusion model for skin lesion delineation,” in <span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">International Workshop on PRedictive Intelligence In MEdicine</span>, pp. 146–158, Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
A. Flaborea, L. Collorone, G. M. D. di Melendugno, S. D’Arrigo, B. Prenkaj, and F. Galasso, “Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 10318–10329, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
J. Wu, R. Fu, H. Fang, Y. Zhang, and Y. Xu, “Medsegdiff-v2: Diffusion based medical image segmentation with transformer,” <span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">arXiv preprint arXiv:2301.11798</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
G. J. Chowdary and Z. Yin, “Diffusion transformer u-net for medical image segmentation,” in <span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>, pp. 622–631, Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
I. Kapelyukh, V. Vosylius, and E. Johns, “Dall-e-bot: Introducing web-scale diffusion models to robotics,” <span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">IEEE Robotics and Automation Letters</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
W. Liu, T. Hermans, S. Chernova, and C. Paxton, “Structdiffusion: Object-centric diffusion for semantic rearrangement of novel objects,” in <span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">Workshop on Language and Robotics at CoRL 2022</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine, “Planning with diffusion for flexible behavior synthesis,” <span class="ltx_text ltx_font_italic" id="bib.bib154.1.1">arXiv preprint arXiv:2205.09991</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal, “Is conditional generative modeling all you need for decision-making?,” <span class="ltx_text ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:2211.15657</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
J. Carvalho, A. T. Le, M. Baierl, D. Koert, and J. Peters, “Motion planning diffusion: Learning and planning of robot motions with diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pp. 1916–1923, IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
X. Gu, C. Wen, J. Song, and Y. Gao, “Seer: Language instructed video prediction with latent diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">arXiv preprint arXiv:2303.14897</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Z. Chen, S. Kiami, A. Gupta, and V. Kumar, “Genaug: Retargeting behaviors to unseen situations via generative augmentation,” <span class="ltx_text ltx_font_italic" id="bib.bib158.1.1">arXiv preprint arXiv:2302.06671</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Rajeswaran, and V. Kumar, “Cacti: A framework for scalable multi-task multi-scene visual imitation learning,” <span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">arXiv preprint arXiv:2212.05711</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
T. Chen, L. Li, S. Saxena, G. Hinton, and D. J. Fleet, “A generalist framework for panoptic segmentation of images and videos,” in <span class="ltx_text ltx_font_italic" id="bib.bib160.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 909–919, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood, “Flexible diffusion modeling of long videos,” <span class="ltx_text ltx_font_italic" id="bib.bib161.1.1">Advances in Neural Information Processing Systems</span>, vol. 35, pp. 27953–27965, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
A. Gupta, S. Tian, Y. Zhang, J. Wu, R. Martín-Martín, and L. Fei-Fei, “Maskvit: Masked visual pre-training for video prediction,” <span class="ltx_text ltx_font_italic" id="bib.bib162.1.1">arXiv preprint arXiv:2206.11894</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, “Cogvideo: Large-scale pretraining for text-to-video generation via transformers,” <span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">arXiv preprint arXiv:2205.15868</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, <span class="ltx_text ltx_font_italic" id="bib.bib164.1.1">et al.</span>, “Make-a-video: Text-to-video generation without text-video data,” <span class="ltx_text ltx_font_italic" id="bib.bib164.2.2">arXiv preprint arXiv:2209.14792</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
D. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng, “Magicvideo: Efficient video generation with latent diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">arXiv preprint arXiv:2211.11018</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
S. Ge, T. Hayes, H. Yang, X. Yin, G. Pang, D. Jacobs, J.-B. Huang, and D. Parikh, “Long video generation with time-agnostic vqgan and time-sensitive transformer,” in <span class="ltx_text ltx_font_italic" id="bib.bib166.1.1">European Conference on Computer Vision</span>, pp. 102–118, Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan, “Phenaki: Variable length video generation from open domain textual description,” <span class="ltx_text ltx_font_italic" id="bib.bib167.1.1">arXiv preprint arXiv:2210.02399</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis, “Structure and content-guided video synthesis with diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 7346–7356, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
L. Khachatryan, A. Movsisyan, V. Tadevosyan, R. Henschel, Z. Wang, S. Navasardyan, and H. Shi, “Text2video-zero: Text-to-image diffusion models are zero-shot video generators,” <span class="ltx_text ltx_font_italic" id="bib.bib169.1.1">arXiv preprint arXiv:2303.13439</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao, J. Zhou, and T. Tan, “Videofusion: Decomposed diffusion models for high-quality video generation,” in <span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 10209–10218, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
A. Jabri, D. Fleet, and T. Chen, “Scalable adaptive computation for iterative generation,” <span class="ltx_text ltx_font_italic" id="bib.bib171.1.1">arXiv preprint arXiv:2212.11972</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
L. Lian, B. Shi, A. Yala, T. Darrell, and B. Li, “Llm-grounded video diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib172.1.1">arXiv preprint arXiv:2309.17444</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
E. Molad, E. Horwitz, D. Valevski, A. R. Acha, Y. Matias, Y. Pritch, Y. Leviathan, and Y. Hoshen, “Dreamix: Video diffusion models are general video editors,” <span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">arXiv preprint arXiv:2302.01329</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
J. H. Liew, H. Yan, J. Zhang, Z. Xu, and J. Feng, “Magicedit: High-fidelity and temporally coherent video editing,” <span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">arXiv preprint arXiv:2308.14749</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
W. Chen, J. Wu, P. Xie, H. Wu, J. Li, X. Xia, X. Xiao, and L. Lin, “Control-a-video: Controllable text-to-video generation with diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib175.1.1">arXiv preprint arXiv:2305.13840</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
W. Chai, X. Guo, G. Wang, and Y. Lu, “Stablevideo: Text-driven consistency-aware diffusion video editing,” in <span class="ltx_text ltx_font_italic" id="bib.bib176.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 23040–23050, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
S. Yang, Y. Zhou, Z. Liu, and C. C. Loy, “Rerender a video: Zero-shot text-guided video-to-video translation,” <span class="ltx_text ltx_font_italic" id="bib.bib177.1.1">arXiv preprint arXiv:2306.07954</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
D. Ceylan, C.-H. P. Huang, and N. J. Mitra, “Pix2video: Video editing using image diffusion,” in <span class="ltx_text ltx_font_italic" id="bib.bib178.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 23206–23217, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
B. Qin, J. Li, S. Tang, T.-S. Chua, and Y. Zhuang, “Instructvid2vid: Controllable video editing with natural language instructions,” <span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">arXiv preprint arXiv:2305.12328</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
D. Liu, Q. Li, A.-D. Dinh, T. Jiang, M. Shah, and C. Xu, “Diffusion action segmentation,” in <span class="ltx_text ltx_font_italic" id="bib.bib180.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 10139–10149, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
R. Feng, Y. Gao, T. H. E. Tse, X. Ma, and H. J. Chang, “Diffpose: Spatiotemporal diffusion model for video-based human pose estimation,” in <span class="ltx_text ltx_font_italic" id="bib.bib181.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 14861–14872, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
L. Yu, Y. Cheng, K. Sohn, J. Lezama, H. Zhang, H. Chang, A. G. Hauptmann, M.-H. Yang, Y. Hao, I. Essa, <span class="ltx_text ltx_font_italic" id="bib.bib182.1.1">et al.</span>, “Magvit: Masked generative video transformer,” in <span class="ltx_text ltx_font_italic" id="bib.bib182.2.2">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 10459–10469, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Z. Li, R. Tucker, N. Snavely, and A. Holynski, “Generative image dynamics,” <span class="ltx_text ltx_font_italic" id="bib.bib183.1.1">arXiv preprint arXiv:2309.07906</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
EasyWithAI, “Zeroscope - ai text-to-video model.” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://easywithai.com/ai-video-generators/zeroscope/" title="">https://easywithai.com/ai-video-generators/zeroscope/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
R. Girdhar, M. Singh, A. Brown, Q. Duval, S. Azadi, S. S. Rambhatla, A. Shah, X. Yin, D. Parikh, and I. Misra, “Emu video: Factorizing text-to-video generation by explicit image conditioning,” <span class="ltx_text ltx_font_italic" id="bib.bib185.1.1">arXiv preprint arXiv:2311.10709</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Y. Zeng, G. Wei, J. Zheng, J. Zou, Y. Wei, Y. Zhang, and H. Li, “Make pixels dance: High-dynamic video generation,” <span class="ltx_text ltx_font_italic" id="bib.bib186.1.1">arXiv preprint arXiv:2311.10982</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, L. Fei-Fei, I. Essa, L. Jiang, and J. Lezama, “Photorealistic video generation with diffusion models,” <span class="ltx_text ltx_font_italic" id="bib.bib187.1.1">arXiv preprint arXiv:2312.06662</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
B. Wu, C.-Y. Chuang, X. Wang, Y. Jia, K. Krishnakumar, T. Xiao, F. Liang, L. Yu, and P. Vajda, “Fairy: Fast parallelized instruction-guided video-to-video synthesis,” <span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">arXiv preprint arXiv:2312.13834</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, R. Hornung, H. Adam, H. Akbari, Y. Alon, V. Birodkar, <span class="ltx_text ltx_font_italic" id="bib.bib189.1.1">et al.</span>, “Videopoet: A large language model for zero-shot video generation,” <span class="ltx_text ltx_font_italic" id="bib.bib189.2.2">arXiv preprint arXiv:2312.14125</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
J. Wu, X. Li, C. Si, S. Zhou, J. Yang, J. Zhang, Y. Li, K. Chen, Y. Tong, Z. Liu, <span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">et al.</span>, “Towards language-driven video inpainting via multimodal large language models,” <span class="ltx_text ltx_font_italic" id="bib.bib190.2.2">arXiv preprint arXiv:2401.10226</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, A. Ephrat, J. Hur, Y. Li, T. Michaeli, <span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">et al.</span>, “Lumiere: A space-time diffusion model for video generation,” <span class="ltx_text ltx_font_italic" id="bib.bib191.2.2">arXiv preprint arXiv:2401.12945</span>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Related Works</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We show some related works about the video generation tasks in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#A1.T1" title="Table 1 ‣ Appendix A Related Works ‣ Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of Video Generation.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T1.1" style="width:390.3pt;height:543.8pt;vertical-align:-1.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-102.7pt,142.7pt) scale(0.655201680216741,0.655201680216741) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T1.1.1">
<tr class="ltx_tr" id="A1.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.1.1.1">Model name</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.1.2.1">Year</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.1.3.1">Backbone</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.1.4.1">Task</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T1.1.1.1.5.1">Group</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.2.1">Imagen Video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib29" title="">29</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.2.2">2022</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.2.3">Diffusion</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.2.4">Generation</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T1.1.1.2.5">Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.3">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.3.1">Pix2Seq-D<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib160" title="">160</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.3.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.3.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.3.4">Segmentation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.3.5">Google Deepmind</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.4">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.4.1">FDM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib161" title="">161</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.4.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.4.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.4.4">Prediction</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.4.5">UBC</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.5">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.5.1">MaskViT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib162" title="">162</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.5.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.5.3">Masked Vision Models</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.5.4">Prediction</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.5.5">Stanford, Salesforce</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.6">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.6.1">CogVideo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib163" title="">163</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.6.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.6.3">Auto-regressive</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.6.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.6.5">THU</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.7">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.7.1">Make-a-video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib164" title="">164</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.7.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.7.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.7.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.7.5">Meta</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.8">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.8.1">MagicVideo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib165" title="">165</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.8.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.8.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.8.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.8.5">ByteDance</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.9">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.9.1">TATS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib166" title="">166</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.9.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.9.3">Auto-regressive</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.9.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.9.5">University of Maryland, Meta</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.10">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.10.1">Phenaki<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib167" title="">167</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.10.2">2022</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.10.3">Masked Vision Models</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.10.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.10.5">Google Brain</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.11">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.11.1">Gen-1<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib168" title="">168</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.11.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.11.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.11.4">Generation, Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.11.5">RunwayML</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.12">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.12.1">LFDM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib140" title="">140</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.12.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.12.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.12.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.12.5">PSU, UCSD</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.13">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.13.1">Text2video-Zero<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib169" title="">169</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.13.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.13.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.13.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.13.5">Picsart</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.14">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.14.1">Video Fusion<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib170" title="">170</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.14.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.14.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.14.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.14.5">USAC, Alibaba</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.15">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.15.1">PYoCo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib34" title="">34</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.15.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.15.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.15.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.15.5">Nvidia</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.16">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.16.1">Video LDM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.16.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.16.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.16.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.16.5">University of Maryland, Nvidia</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.17">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.17.1">RIN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib171" title="">171</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.17.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.17.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.17.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.17.5">Google Brain</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.18">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.18.1">LVD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib172" title="">172</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.18.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.18.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.18.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.18.5">UCB</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.19">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.19.1">Dreamix<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib173" title="">173</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.19.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.19.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.19.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.19.5">Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.20">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.20.1">MagicEdit<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib174" title="">174</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.20.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.20.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.20.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.20.5">ByteDance</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.21">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.21.1">Control-A-Video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib175" title="">175</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.21.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.21.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.21.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.21.5">Sun Yat-Sen University</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.22">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.22.1">StableVideo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib176" title="">176</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.22.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.22.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.22.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.22.5">ZJU, MSRA</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.23">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.23.1">Tune-A-Video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib78" title="">78</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.23.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.23.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.23.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.23.5">NUS</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.24">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.24.1">Rerender-A-Video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib177" title="">177</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.24.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.24.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.24.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.24.5">NTU</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.25">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.25.1">Pix2Video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib178" title="">178</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.25.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.25.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.25.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.25.5">Adobe, UCL</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.26">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.26.1">InstructVid2Vid<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib179" title="">179</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.26.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.26.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.26.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.26.5">ZJU</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.27">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.27.1">DiffAct<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib180" title="">180</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.27.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.27.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.27.4">Action Detection</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.27.5">University of Sydney</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.28">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.28.1">DiffPose<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib181" title="">181</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.28.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.28.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.28.4">Pose Estimation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.28.5">Jilin University</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.29">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.29.1">MAGVIT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib182" title="">182</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.29.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.29.3">Masked Vision Models</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.29.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.29.5">Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.30">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.30.1">AnimateDiff<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib138" title="">138</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.30.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.30.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.30.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.30.5">CUHK</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.31">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.31.1">MAGVIT V2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib47" title="">47</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.31.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.31.3">Masked Vision Models</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.31.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.31.5">Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.32">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.32.1">Generative Dynamics<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib183" title="">183</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.32.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.32.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.32.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.32.5">Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.33">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.33.1">VideoCrafter<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib81" title="">81</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.33.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.33.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.33.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.33.5">Tencent</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.34">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.34.1">Zeroscope<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib184" title="">184</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.34.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.34.3">-</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.34.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.34.5">EasyWithAI</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.35">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.35.1">ModelScope</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.35.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.35.3">-</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.35.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.35.5">Damo</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.36">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.36.1">Gen-2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib23" title="">23</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.36.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.36.3">-</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.36.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.36.5">RunwayML</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.37">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.37.1">Pika<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.37.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.37.3">-</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.37.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.37.5">Pika Labs</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.38">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.38.1">Emu Video<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib185" title="">185</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.38.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.38.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.38.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.38.5">Meta</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.39">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.39.1">PixelDance<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib186" title="">186</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.39.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.39.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.39.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.39.5">ByteDance</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.40">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.40.1">Stable Video Diffusion<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.40.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.40.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.40.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.40.5">Stability AI</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.41">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.41.1">W.A.L.T<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib187" title="">187</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.41.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.41.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.41.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.41.5">Stanford, Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.42">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.42.1">Fairy<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib188" title="">188</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.42.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.42.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.42.4">Generation, Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.42.5">Meta</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.43">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.43.1">VideoPoet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib189" title="">189</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.43.2">2023</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.43.3">Auto-regressive</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.43.4">Generation, Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.43.5">Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.44">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.44.1">LGVI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib190" title="">190</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.44.2">2024</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.44.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.44.4">Editing</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.44.5">PKU, NTU</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.45">
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.45.1">Lumiere<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib191" title="">191</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.45.2">2024</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.45.3">Diffusion</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.45.4">Generation</td>
<td class="ltx_td ltx_align_left" id="A1.T1.1.1.45.5">Google</td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.1.46">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.1.1.46.1">Sora<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17177v1#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.1.1.46.2">2024</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.1.1.46.3">Diffusion</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.1.1.46.4">Generation, Editing</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T1.1.1.46.5">OpenAI</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Feb 26 20:42:26 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
