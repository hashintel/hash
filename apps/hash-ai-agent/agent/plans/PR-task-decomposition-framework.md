# PR: Task Decomposition & Planning Framework

## ğŸŒŸ What is the purpose of this PR?

Introduces a framework for decomposing complex R&D goals into structured, executable plans using LLM-based planning agents. The core insight is treating LLM planning as a "compiler front-end" that produces an **Intermediate Representation (IR)** â€” the `PlanSpec` â€” which can be validated, scored, and eventually compiled into executable workflows.

This PR establishes the foundational infrastructure for plan generation and quality evaluation, with the goal of enabling autonomous research and development workflows.

## ğŸ”— Related links

- `agent/docs/PLAN-task-decomposition.md` â€” Full design document and implementation plan
- `agent/docs/E2E-test-results-2024-12-17.md` â€” Latest E2E test outputs

## ğŸš« Blocked by

_None_

## ğŸ” What does this change?

### Core Schema & Types

- **`schemas/plan-spec.ts`** â€” Full Zod schema for `PlanSpec` with 4 step types:
  - `research` â€” Parallelizable information gathering
  - `synthesize` â€” Combining findings (integrative) or evaluating results (evaluative)
  - `experiment` â€” Testing hypotheses (exploratory or confirmatory with preregistration)
  - `develop` â€” Building/implementing artifacts

- **`schemas/planning-fixture.ts`** â€” Types for test fixtures (`PlanningFixture`, `ExpectedPlanCharacteristics`)

- **`constants.ts`** â€” 12 agent capability profiles with `canHandle` mappings for executor assignment

### Validation & Analysis

- **`tools/plan-validator.ts`** â€” 12 structural validation checks:
  - DAG validity (no cycles, valid references)
  - Executor compatibility
  - Preregistration requirements for confirmatory experiments
  - Input/output consistency

- **`tools/topology-analyzer.ts`** â€” DAG analysis utilities:
  - Entry/exit point detection
  - Critical path calculation
  - Parallel group identification

### Scoring System

- **`scorers/plan-scorers.ts`** â€” 4 deterministic scorers (no LLM, fast):
  - `scorePlanStructure` â€” DAG validity, parallelism, step type diversity
  - `scorePlanCoverage` â€” Requirement/hypothesis coverage
  - `scoreExperimentRigor` â€” Preregistration, success criteria
  - `scoreUnknownsCoverage` â€” Epistemic completeness

- **`scorers/plan-llm-scorers.ts`** â€” 3 LLM-based judges:
  - `goalAlignmentScorer` â€” Does plan address the goal?
  - `planGranularityScorer` â€” Are steps appropriately sized?
  - `hypothesisTestabilityScorer` â€” Are hypotheses testable?

### Planning Agent

- **`agents/planner-agent.ts`** â€” `generatePlan(goal, context)` function that uses structured output to produce valid `PlanSpec` instances

### Test Fixtures

4 fixtures of increasing complexity in `fixtures/decomposition-prompts/`:

| Fixture | Complexity | Step Types |
|---------|------------|------------|
| `summarize-papers` | Simple linear | research â†’ synthesize |
| `explore-and-recommend` | Parallel research | research (parallel) â†’ synthesize (evaluative) |
| `hypothesis-validation` | With experiments | research â†’ experiment â†’ synthesize |
| `ct-database-goal` | Full R&D cycle | All 4 types, hypotheses, experiments |

### E2E Test Suite

- **`workflows/planning-workflow.test.ts`** â€” Comprehensive E2E tests:
  - Runs all 4 fixtures through the full pipeline
  - Validates generated plans
  - Runs deterministic scorers
  - Optional LLM scorers via `RUN_LLM_SCORERS=true`
  - Generates summary report with score table

## Pre-Merge Checklist ğŸš€

### ğŸš¢ Has this modified a publishable library?

This PR:

- [x] does not modify any publishable blocks or libraries, or modifications do not need publishing

### ğŸ“œ Does this require a change to the docs?

The changes in this PR:

- [x] are internal and do not require a docs change

### ğŸ•¸ï¸ Does this require a change to the Turbo Graph?

The changes in this PR:

- [x] do not affect the execution graph

## âš ï¸ Known issues

1. **ct-database-goal fixture fails validation** â€” The LLM occasionally generates confirmatory experiments without `preregisteredCommitments`. This is a known prompt engineering issue that will be addressed in the revision workflow.

2. **explore-and-recommend generates unexpected content** â€” The LLM adds hypotheses and experiments not specified in the fixture expectations. This is valid behavior (more thorough than minimum), but indicates fixture expectations may need adjustment.

## ğŸ¾ Next steps

Per `PLAN-task-decomposition.md` Section 18:

1. **Revision workflow loop** â€” Implement `dountil` loop: generate â†’ validate â†’ feedback â†’ regenerate (max 3 attempts)
2. **Supervisor agent** â€” LLM approval gate before plan finalization
3. **Prompt improvements** â€” Strengthen preregisteredCommitments requirement
4. **Stub execution** â€” Low priority, deferred

## ğŸ›¡ What tests cover this?

- `plan-validator.test.ts` â€” 25 negative fixture tests for validation
- `plan-scorers.test.ts` â€” 23 unit tests for deterministic scorers
- `plan-llm-scorers.test.ts` â€” 6 tests for LLM judges
- `fixtures.test.ts` â€” 4 fixture validation tests
- `planning-workflow.test.ts` â€” E2E pipeline tests (3/4 passing)

## â“ How to test this?

1. Checkout the branch
2. `cd apps/hash-ai-agent`
3. Run unit tests: `npx vitest run src/mastra/scorers/plan-scorers.test.ts`
4. Run E2E tests: `npx vitest run src/mastra/workflows/planning-workflow.test.ts`
5. (Optional) Run with LLM scorers: `RUN_LLM_SCORERS=true npx vitest run src/mastra/workflows/planning-workflow.test.ts`

## ğŸ“¹ Demo

See `agent/docs/E2E-test-results-2024-12-17.md` for full test output, including:

```
Deterministic Scores:
  Fixture                     | Overall | Structure | Coverage | Rigor | Unknowns
  -------------------------------------------------------------------------------------
  summarize-papers             |     93% |       77% |     100% |  100% |      93%
  explore-and-recommend        |     92% |       86% |      93% |   93% |     100%
  hypothesis-validation        |     95% |       86% |     100% |   95% |     100%
```
