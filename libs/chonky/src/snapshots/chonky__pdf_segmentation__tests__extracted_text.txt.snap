---
source: libs/chonky/src/lib.rs
expression: text
snapshot_kind: text
---
Published as a conference paper at ICLR 2021Table 2: Comparison with state of the art on popular image classification benchmarks. We report mean and standard deviation of the accuracies, averaged over three fine-tuning runs. VisionTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on alldatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on thesmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reportedin Touvron et al. (2020).Figure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.model still took substantially less compute to pre-train than prior state of the art. However, we notethat pre-training efficiency may be affected not only by the architecture choice, but also other parameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study ofperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 modelpre-trained on the public ImageNet-21k dataset performs well on most datasets too, while takingfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days.Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTAmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannenet al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).ViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On theSpecialized the performance of the top two models is similar.4.3 PRE-TRAINING DATA REQUIREMENTSThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewerinductive biases for vision than ResNets, how crucial is the dataset size? We perform two series ofexperiments.First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT300M. To boost the performance on the smaller datasets, we optimize three basic regularizationparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after finetuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on thesmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Onlywith JFT-300M, do we see the full benefit of larger models. Figure 3 also shows the performance2Note that the ImageNet pre-trained models are also fine-tuned, but again on ImageNet. This is because theresolution increase during fine-tuning improves the performance.6
